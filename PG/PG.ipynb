{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PG.agent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load per hour bitcoin data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data proccess\n",
    "DATA_PATH = './per_min_data/'\n",
    "\n",
    "data = pd.read_csv(DATA_PATH + os.listdir(DATA_PATH)[0])\n",
    "\n",
    "data.iloc[:,6] = pd.to_datetime(data.iloc[:,6], unit='ms').apply(lambda x: x.strftime('%Y-%m-%d %H:%M'))\n",
    "data.iloc[:,6] = pd.to_datetime(data.iloc[:,6])\n",
    "data.set_index('Close time', inplace=True)\n",
    "data = data.iloc[:,1:6]\n",
    "# Aggregate per hour\n",
    "data = data.resample('60T').agg({\n",
    "    'Open':'first',\n",
    "    'High':'max',\n",
    "    'Low':'min',\n",
    "    'Close':'last',\n",
    "    'Volume':'sum'\n",
    "    })\n",
    "data = data.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-04-27 14:00:00</th>\n",
       "      <td>9226.97</td>\n",
       "      <td>9290.00</td>\n",
       "      <td>9202.00</td>\n",
       "      <td>9288.88</td>\n",
       "      <td>1401.058675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 15:00:00</th>\n",
       "      <td>9280.14</td>\n",
       "      <td>9288.88</td>\n",
       "      <td>9210.01</td>\n",
       "      <td>9241.21</td>\n",
       "      <td>1359.959170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 16:00:00</th>\n",
       "      <td>9241.20</td>\n",
       "      <td>9328.26</td>\n",
       "      <td>9241.00</td>\n",
       "      <td>9282.10</td>\n",
       "      <td>1395.512101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 17:00:00</th>\n",
       "      <td>9286.01</td>\n",
       "      <td>9319.99</td>\n",
       "      <td>9266.00</td>\n",
       "      <td>9290.10</td>\n",
       "      <td>1122.230082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 18:00:00</th>\n",
       "      <td>9290.10</td>\n",
       "      <td>9296.31</td>\n",
       "      <td>9213.80</td>\n",
       "      <td>9230.89</td>\n",
       "      <td>1001.907142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 19:00:00</th>\n",
       "      <td>9230.89</td>\n",
       "      <td>9250.56</td>\n",
       "      <td>9075.01</td>\n",
       "      <td>9089.00</td>\n",
       "      <td>2097.185440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 20:00:00</th>\n",
       "      <td>9089.12</td>\n",
       "      <td>9163.38</td>\n",
       "      <td>8940.00</td>\n",
       "      <td>8970.00</td>\n",
       "      <td>3084.066108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 21:00:00</th>\n",
       "      <td>8969.99</td>\n",
       "      <td>9053.00</td>\n",
       "      <td>8909.83</td>\n",
       "      <td>8975.35</td>\n",
       "      <td>1911.233654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 22:00:00</th>\n",
       "      <td>8975.36</td>\n",
       "      <td>9093.09</td>\n",
       "      <td>8975.36</td>\n",
       "      <td>9031.00</td>\n",
       "      <td>1077.087976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-27 23:00:00</th>\n",
       "      <td>9032.00</td>\n",
       "      <td>9073.93</td>\n",
       "      <td>8889.00</td>\n",
       "      <td>8915.35</td>\n",
       "      <td>1324.768951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 00:00:00</th>\n",
       "      <td>8915.35</td>\n",
       "      <td>9049.76</td>\n",
       "      <td>8870.00</td>\n",
       "      <td>9010.11</td>\n",
       "      <td>1754.040320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 01:00:00</th>\n",
       "      <td>9005.98</td>\n",
       "      <td>9126.81</td>\n",
       "      <td>9000.00</td>\n",
       "      <td>9126.81</td>\n",
       "      <td>1298.861448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 02:00:00</th>\n",
       "      <td>9126.81</td>\n",
       "      <td>9129.17</td>\n",
       "      <td>9050.00</td>\n",
       "      <td>9055.78</td>\n",
       "      <td>1317.636058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 03:00:00</th>\n",
       "      <td>9053.47</td>\n",
       "      <td>9083.30</td>\n",
       "      <td>9051.00</td>\n",
       "      <td>9080.00</td>\n",
       "      <td>59.260592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 06:00:00</th>\n",
       "      <td>9182.00</td>\n",
       "      <td>9215.66</td>\n",
       "      <td>9150.67</td>\n",
       "      <td>9163.03</td>\n",
       "      <td>467.011505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 07:00:00</th>\n",
       "      <td>9163.02</td>\n",
       "      <td>9171.07</td>\n",
       "      <td>9108.99</td>\n",
       "      <td>9126.22</td>\n",
       "      <td>1203.100815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 08:00:00</th>\n",
       "      <td>9121.05</td>\n",
       "      <td>9330.00</td>\n",
       "      <td>9081.02</td>\n",
       "      <td>9217.48</td>\n",
       "      <td>3048.753373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 09:00:00</th>\n",
       "      <td>9227.49</td>\n",
       "      <td>9300.40</td>\n",
       "      <td>9155.06</td>\n",
       "      <td>9273.00</td>\n",
       "      <td>1973.035083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 10:00:00</th>\n",
       "      <td>9273.00</td>\n",
       "      <td>9310.00</td>\n",
       "      <td>9225.00</td>\n",
       "      <td>9277.29</td>\n",
       "      <td>1357.758118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 11:00:00</th>\n",
       "      <td>9277.29</td>\n",
       "      <td>9287.56</td>\n",
       "      <td>9190.00</td>\n",
       "      <td>9218.57</td>\n",
       "      <td>754.879535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 12:00:00</th>\n",
       "      <td>9218.57</td>\n",
       "      <td>9400.01</td>\n",
       "      <td>9210.00</td>\n",
       "      <td>9378.07</td>\n",
       "      <td>2380.448524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 13:00:00</th>\n",
       "      <td>9378.07</td>\n",
       "      <td>9419.00</td>\n",
       "      <td>9334.00</td>\n",
       "      <td>9387.99</td>\n",
       "      <td>1426.644910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 14:00:00</th>\n",
       "      <td>9387.99</td>\n",
       "      <td>9427.48</td>\n",
       "      <td>9280.00</td>\n",
       "      <td>9311.02</td>\n",
       "      <td>2407.899203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 15:00:00</th>\n",
       "      <td>9310.25</td>\n",
       "      <td>9361.86</td>\n",
       "      <td>9239.00</td>\n",
       "      <td>9360.00</td>\n",
       "      <td>1694.235994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 16:00:00</th>\n",
       "      <td>9360.00</td>\n",
       "      <td>9384.40</td>\n",
       "      <td>9260.00</td>\n",
       "      <td>9316.29</td>\n",
       "      <td>1646.391503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 17:00:00</th>\n",
       "      <td>9316.29</td>\n",
       "      <td>9365.35</td>\n",
       "      <td>9287.00</td>\n",
       "      <td>9290.00</td>\n",
       "      <td>1681.816133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 18:00:00</th>\n",
       "      <td>9291.00</td>\n",
       "      <td>9410.00</td>\n",
       "      <td>9290.00</td>\n",
       "      <td>9390.52</td>\n",
       "      <td>1484.448022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 19:00:00</th>\n",
       "      <td>9398.98</td>\n",
       "      <td>9404.99</td>\n",
       "      <td>9350.90</td>\n",
       "      <td>9393.00</td>\n",
       "      <td>836.800743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 20:00:00</th>\n",
       "      <td>9393.00</td>\n",
       "      <td>9427.48</td>\n",
       "      <td>9300.00</td>\n",
       "      <td>9331.58</td>\n",
       "      <td>1302.673315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-28 21:00:00</th>\n",
       "      <td>9313.73</td>\n",
       "      <td>9350.23</td>\n",
       "      <td>9258.00</td>\n",
       "      <td>9300.00</td>\n",
       "      <td>1234.697064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 12:00:00</th>\n",
       "      <td>3429.62</td>\n",
       "      <td>3470.00</td>\n",
       "      <td>3428.72</td>\n",
       "      <td>3457.74</td>\n",
       "      <td>2816.208139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 13:00:00</th>\n",
       "      <td>3457.79</td>\n",
       "      <td>3476.36</td>\n",
       "      <td>3452.00</td>\n",
       "      <td>3458.82</td>\n",
       "      <td>2350.644001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 14:00:00</th>\n",
       "      <td>3458.90</td>\n",
       "      <td>3463.89</td>\n",
       "      <td>3450.00</td>\n",
       "      <td>3462.45</td>\n",
       "      <td>1260.303697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 15:00:00</th>\n",
       "      <td>3462.64</td>\n",
       "      <td>3468.00</td>\n",
       "      <td>3450.21</td>\n",
       "      <td>3464.01</td>\n",
       "      <td>994.558197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 16:00:00</th>\n",
       "      <td>3464.03</td>\n",
       "      <td>3470.00</td>\n",
       "      <td>3454.97</td>\n",
       "      <td>3458.56</td>\n",
       "      <td>1158.854429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 17:00:00</th>\n",
       "      <td>3459.50</td>\n",
       "      <td>3469.00</td>\n",
       "      <td>3458.54</td>\n",
       "      <td>3465.60</td>\n",
       "      <td>710.460860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 18:00:00</th>\n",
       "      <td>3465.74</td>\n",
       "      <td>3478.00</td>\n",
       "      <td>3462.62</td>\n",
       "      <td>3469.70</td>\n",
       "      <td>867.683081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 19:00:00</th>\n",
       "      <td>3469.72</td>\n",
       "      <td>3470.87</td>\n",
       "      <td>3456.74</td>\n",
       "      <td>3458.77</td>\n",
       "      <td>728.274503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 20:00:00</th>\n",
       "      <td>3458.65</td>\n",
       "      <td>3460.77</td>\n",
       "      <td>3446.82</td>\n",
       "      <td>3453.70</td>\n",
       "      <td>1030.770651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 21:00:00</th>\n",
       "      <td>3453.70</td>\n",
       "      <td>3457.50</td>\n",
       "      <td>3448.04</td>\n",
       "      <td>3456.27</td>\n",
       "      <td>410.138002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 22:00:00</th>\n",
       "      <td>3456.25</td>\n",
       "      <td>3468.63</td>\n",
       "      <td>3454.42</td>\n",
       "      <td>3468.37</td>\n",
       "      <td>385.756504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30 23:00:00</th>\n",
       "      <td>3468.49</td>\n",
       "      <td>3469.00</td>\n",
       "      <td>3452.00</td>\n",
       "      <td>3458.18</td>\n",
       "      <td>556.859658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 00:00:00</th>\n",
       "      <td>3457.50</td>\n",
       "      <td>3489.20</td>\n",
       "      <td>3456.75</td>\n",
       "      <td>3480.71</td>\n",
       "      <td>1148.326231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 01:00:00</th>\n",
       "      <td>3481.18</td>\n",
       "      <td>3481.70</td>\n",
       "      <td>3468.00</td>\n",
       "      <td>3472.65</td>\n",
       "      <td>763.633559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 02:00:00</th>\n",
       "      <td>3471.36</td>\n",
       "      <td>3476.98</td>\n",
       "      <td>3462.97</td>\n",
       "      <td>3470.68</td>\n",
       "      <td>780.394866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 03:00:00</th>\n",
       "      <td>3470.63</td>\n",
       "      <td>3472.63</td>\n",
       "      <td>3463.46</td>\n",
       "      <td>3465.32</td>\n",
       "      <td>473.973271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 04:00:00</th>\n",
       "      <td>3465.32</td>\n",
       "      <td>3467.77</td>\n",
       "      <td>3455.30</td>\n",
       "      <td>3466.20</td>\n",
       "      <td>794.965741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 05:00:00</th>\n",
       "      <td>3467.70</td>\n",
       "      <td>3470.81</td>\n",
       "      <td>3465.00</td>\n",
       "      <td>3466.94</td>\n",
       "      <td>802.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 06:00:00</th>\n",
       "      <td>3465.77</td>\n",
       "      <td>3478.00</td>\n",
       "      <td>3464.07</td>\n",
       "      <td>3473.00</td>\n",
       "      <td>532.478240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 07:00:00</th>\n",
       "      <td>3473.02</td>\n",
       "      <td>3480.00</td>\n",
       "      <td>3466.52</td>\n",
       "      <td>3476.57</td>\n",
       "      <td>744.647213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 08:00:00</th>\n",
       "      <td>3477.08</td>\n",
       "      <td>3477.08</td>\n",
       "      <td>3432.02</td>\n",
       "      <td>3442.98</td>\n",
       "      <td>2381.290685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 09:00:00</th>\n",
       "      <td>3442.98</td>\n",
       "      <td>3449.00</td>\n",
       "      <td>3426.70</td>\n",
       "      <td>3444.56</td>\n",
       "      <td>1897.078062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 10:00:00</th>\n",
       "      <td>3444.56</td>\n",
       "      <td>3454.23</td>\n",
       "      <td>3418.80</td>\n",
       "      <td>3425.70</td>\n",
       "      <td>5387.615012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 11:00:00</th>\n",
       "      <td>3425.69</td>\n",
       "      <td>3434.14</td>\n",
       "      <td>3420.01</td>\n",
       "      <td>3427.81</td>\n",
       "      <td>2364.034575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 12:00:00</th>\n",
       "      <td>3427.79</td>\n",
       "      <td>3444.88</td>\n",
       "      <td>3420.30</td>\n",
       "      <td>3437.62</td>\n",
       "      <td>1848.117485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 13:00:00</th>\n",
       "      <td>3437.36</td>\n",
       "      <td>3444.00</td>\n",
       "      <td>3433.63</td>\n",
       "      <td>3441.62</td>\n",
       "      <td>1056.436893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 14:00:00</th>\n",
       "      <td>3441.62</td>\n",
       "      <td>3441.62</td>\n",
       "      <td>3423.06</td>\n",
       "      <td>3429.00</td>\n",
       "      <td>1206.040630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 15:00:00</th>\n",
       "      <td>3429.28</td>\n",
       "      <td>3440.70</td>\n",
       "      <td>3427.16</td>\n",
       "      <td>3436.56</td>\n",
       "      <td>762.436383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 16:00:00</th>\n",
       "      <td>3435.82</td>\n",
       "      <td>3440.81</td>\n",
       "      <td>3426.01</td>\n",
       "      <td>3427.20</td>\n",
       "      <td>899.702027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31 17:00:00</th>\n",
       "      <td>3427.20</td>\n",
       "      <td>3436.04</td>\n",
       "      <td>3425.00</td>\n",
       "      <td>3428.30</td>\n",
       "      <td>854.340590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6670 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open     High      Low    Close       Volume\n",
       "Close time                                                          \n",
       "2018-04-27 14:00:00  9226.97  9290.00  9202.00  9288.88  1401.058675\n",
       "2018-04-27 15:00:00  9280.14  9288.88  9210.01  9241.21  1359.959170\n",
       "2018-04-27 16:00:00  9241.20  9328.26  9241.00  9282.10  1395.512101\n",
       "2018-04-27 17:00:00  9286.01  9319.99  9266.00  9290.10  1122.230082\n",
       "2018-04-27 18:00:00  9290.10  9296.31  9213.80  9230.89  1001.907142\n",
       "2018-04-27 19:00:00  9230.89  9250.56  9075.01  9089.00  2097.185440\n",
       "2018-04-27 20:00:00  9089.12  9163.38  8940.00  8970.00  3084.066108\n",
       "2018-04-27 21:00:00  8969.99  9053.00  8909.83  8975.35  1911.233654\n",
       "2018-04-27 22:00:00  8975.36  9093.09  8975.36  9031.00  1077.087976\n",
       "2018-04-27 23:00:00  9032.00  9073.93  8889.00  8915.35  1324.768951\n",
       "2018-04-28 00:00:00  8915.35  9049.76  8870.00  9010.11  1754.040320\n",
       "2018-04-28 01:00:00  9005.98  9126.81  9000.00  9126.81  1298.861448\n",
       "2018-04-28 02:00:00  9126.81  9129.17  9050.00  9055.78  1317.636058\n",
       "2018-04-28 03:00:00  9053.47  9083.30  9051.00  9080.00    59.260592\n",
       "2018-04-28 06:00:00  9182.00  9215.66  9150.67  9163.03   467.011505\n",
       "2018-04-28 07:00:00  9163.02  9171.07  9108.99  9126.22  1203.100815\n",
       "2018-04-28 08:00:00  9121.05  9330.00  9081.02  9217.48  3048.753373\n",
       "2018-04-28 09:00:00  9227.49  9300.40  9155.06  9273.00  1973.035083\n",
       "2018-04-28 10:00:00  9273.00  9310.00  9225.00  9277.29  1357.758118\n",
       "2018-04-28 11:00:00  9277.29  9287.56  9190.00  9218.57   754.879535\n",
       "2018-04-28 12:00:00  9218.57  9400.01  9210.00  9378.07  2380.448524\n",
       "2018-04-28 13:00:00  9378.07  9419.00  9334.00  9387.99  1426.644910\n",
       "2018-04-28 14:00:00  9387.99  9427.48  9280.00  9311.02  2407.899203\n",
       "2018-04-28 15:00:00  9310.25  9361.86  9239.00  9360.00  1694.235994\n",
       "2018-04-28 16:00:00  9360.00  9384.40  9260.00  9316.29  1646.391503\n",
       "2018-04-28 17:00:00  9316.29  9365.35  9287.00  9290.00  1681.816133\n",
       "2018-04-28 18:00:00  9291.00  9410.00  9290.00  9390.52  1484.448022\n",
       "2018-04-28 19:00:00  9398.98  9404.99  9350.90  9393.00   836.800743\n",
       "2018-04-28 20:00:00  9393.00  9427.48  9300.00  9331.58  1302.673315\n",
       "2018-04-28 21:00:00  9313.73  9350.23  9258.00  9300.00  1234.697064\n",
       "...                      ...      ...      ...      ...          ...\n",
       "2019-01-30 12:00:00  3429.62  3470.00  3428.72  3457.74  2816.208139\n",
       "2019-01-30 13:00:00  3457.79  3476.36  3452.00  3458.82  2350.644001\n",
       "2019-01-30 14:00:00  3458.90  3463.89  3450.00  3462.45  1260.303697\n",
       "2019-01-30 15:00:00  3462.64  3468.00  3450.21  3464.01   994.558197\n",
       "2019-01-30 16:00:00  3464.03  3470.00  3454.97  3458.56  1158.854429\n",
       "2019-01-30 17:00:00  3459.50  3469.00  3458.54  3465.60   710.460860\n",
       "2019-01-30 18:00:00  3465.74  3478.00  3462.62  3469.70   867.683081\n",
       "2019-01-30 19:00:00  3469.72  3470.87  3456.74  3458.77   728.274503\n",
       "2019-01-30 20:00:00  3458.65  3460.77  3446.82  3453.70  1030.770651\n",
       "2019-01-30 21:00:00  3453.70  3457.50  3448.04  3456.27   410.138002\n",
       "2019-01-30 22:00:00  3456.25  3468.63  3454.42  3468.37   385.756504\n",
       "2019-01-30 23:00:00  3468.49  3469.00  3452.00  3458.18   556.859658\n",
       "2019-01-31 00:00:00  3457.50  3489.20  3456.75  3480.71  1148.326231\n",
       "2019-01-31 01:00:00  3481.18  3481.70  3468.00  3472.65   763.633559\n",
       "2019-01-31 02:00:00  3471.36  3476.98  3462.97  3470.68   780.394866\n",
       "2019-01-31 03:00:00  3470.63  3472.63  3463.46  3465.32   473.973271\n",
       "2019-01-31 04:00:00  3465.32  3467.77  3455.30  3466.20   794.965741\n",
       "2019-01-31 05:00:00  3467.70  3470.81  3465.00  3466.94   802.463200\n",
       "2019-01-31 06:00:00  3465.77  3478.00  3464.07  3473.00   532.478240\n",
       "2019-01-31 07:00:00  3473.02  3480.00  3466.52  3476.57   744.647213\n",
       "2019-01-31 08:00:00  3477.08  3477.08  3432.02  3442.98  2381.290685\n",
       "2019-01-31 09:00:00  3442.98  3449.00  3426.70  3444.56  1897.078062\n",
       "2019-01-31 10:00:00  3444.56  3454.23  3418.80  3425.70  5387.615012\n",
       "2019-01-31 11:00:00  3425.69  3434.14  3420.01  3427.81  2364.034575\n",
       "2019-01-31 12:00:00  3427.79  3444.88  3420.30  3437.62  1848.117485\n",
       "2019-01-31 13:00:00  3437.36  3444.00  3433.63  3441.62  1056.436893\n",
       "2019-01-31 14:00:00  3441.62  3441.62  3423.06  3429.00  1206.040630\n",
       "2019-01-31 15:00:00  3429.28  3440.70  3427.16  3436.56   762.436383\n",
       "2019-01-31 16:00:00  3435.82  3440.81  3426.01  3427.20   899.702027\n",
       "2019-01-31 17:00:00  3427.20  3436.04  3425.00  3428.30   854.340590\n",
       "\n",
       "[6670 rows x 5 columns]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train data:  5336 number of validation data:  400 number of test data:  800\n",
      "trajectory length is:  100\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(data, train_frac = 0.8, traj_len = 100, batch_size = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 50, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 50, 500)           1012000   \n",
      "_________________________________________________________________\n",
      "raw_policy (TimeDistributed) (None, 50, 2)             1002      \n",
      "=================================================================\n",
      "Total params: 1,013,002\n",
      "Trainable params: 1,013,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9226.97    , 9290.      , 9202.      , 9288.88    , 1401.058675],\n",
       "       [9280.14    , 9288.88    , 9210.01    , 9241.21    , 1359.95917 ],\n",
       "       [9241.2     , 9328.26    , 9241.      , 9282.1     , 1395.512101],\n",
       "       ...,\n",
       "       [3429.28    , 3440.7     , 3427.16    , 3436.56    ,  762.436383],\n",
       "       [3435.82    , 3440.81    , 3426.01    , 3427.2     ,  899.702027],\n",
       "       [3427.2     , 3436.04    , 3425.      , 3428.3     ,  854.34059 ]])"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.data_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.67394871e-02,  2.79946256e-02,  2.69220158e-02,\n",
       "         2.70825929e-02, -2.92987688e-04],\n",
       "       [ 8.44184603e-01,  1.00769187e-02,  1.50293793e-01,\n",
       "        -7.05745688e-01, -6.28594020e-02],\n",
       "       [-5.71470332e-01,  6.56700226e-01,  5.03228309e-01,\n",
       "         6.55912025e-01,  5.39377401e-02],\n",
       "       ...,\n",
       "       [-4.84272813e-01, -1.17370520e-02,  1.96654070e-01,\n",
       "         3.40757403e-01, -9.63957730e-01],\n",
       "       [ 2.97796324e-01,  3.27457114e-02, -2.06652573e-02,\n",
       "        -3.61378626e-01,  3.47584810e-01],\n",
       "       [-3.30633387e-01, -1.78169374e-01, -1.48852009e-02,\n",
       "         7.27900640e-02, -1.09006948e-01]])"
      ]
     },
     "execution_count": 972,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.data_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random sampling 13 trajectors per epoch.\n",
      "epoch 0...\n",
      "step 0, loss: 38.458457946777344, sharpe ratio: 0.03314501536196898\n",
      "step 1, loss: 38.47896194458008, sharpe ratio: 0.11186135967392817\n",
      "step 2, loss: 38.383079528808594, sharpe ratio: -0.07590516853827635\n",
      "step 3, loss: 38.36991500854492, sharpe ratio: -0.046139434672991864\n",
      "step 4, loss: 38.38270568847656, sharpe ratio: -0.007817099523337276\n",
      "step 5, loss: 38.33241653442383, sharpe ratio: -0.059853017514033616\n",
      "step 6, loss: 38.3465690612793, sharpe ratio: -0.00764144639093835\n",
      "step 7, loss: 38.37559127807617, sharpe ratio: 0.06684017845131147\n",
      "step 8, loss: 38.294734954833984, sharpe ratio: -0.06593218863083788\n",
      "step 9, loss: 38.27760314941406, sharpe ratio: -0.06595034532903933\n",
      "step 10, loss: 38.29530715942383, sharpe ratio: -0.013842621786559116\n",
      "step 11, loss: 38.295989990234375, sharpe ratio: -0.002260642233746819\n",
      "step 12, loss: 38.27686309814453, sharpe ratio: 0.007181447137809385\n",
      "validation sharpe ratio: 0.04280213564031686\n",
      "epoch 0: loss: 38.35139846801758, learning rate: 9.999998292187229e-05\n",
      "epoch 1...\n",
      "step 0, loss: 38.264801025390625, sharpe ratio: -0.008523701001694264\n",
      "step 1, loss: 38.2532958984375, sharpe ratio: 0.003403035283736724\n",
      "step 2, loss: 38.204254150390625, sharpe ratio: -0.05193397234432263\n",
      "step 3, loss: 38.262115478515625, sharpe ratio: 0.08458947856855698\n",
      "step 4, loss: 38.23487854003906, sharpe ratio: 0.03982509358713388\n",
      "step 5, loss: 38.21822738647461, sharpe ratio: 0.03832260957827331\n",
      "step 6, loss: 38.1601676940918, sharpe ratio: -0.03261381655680368\n",
      "step 7, loss: 38.17262649536133, sharpe ratio: 0.01652809813810325\n",
      "step 8, loss: 38.13037872314453, sharpe ratio: -0.04397222755829104\n",
      "step 9, loss: 38.127479553222656, sharpe ratio: -0.014365141780043682\n",
      "step 10, loss: 38.1225471496582, sharpe ratio: -0.014889365039064868\n",
      "step 11, loss: 38.131351470947266, sharpe ratio: 0.03058942664290637\n",
      "step 12, loss: 38.13214874267578, sharpe ratio: 0.0653855123276848\n",
      "validation sharpe ratio: 0.015997972046937148\n",
      "epoch 1: loss: 38.18571472167969, learning rate: 9.999998292187229e-05\n",
      "epoch 2...\n",
      "step 0, loss: 38.10462951660156, sharpe ratio: 0.032817147955635034\n",
      "step 1, loss: 38.10346603393555, sharpe ratio: 0.05974935114798025\n",
      "step 2, loss: 38.0644645690918, sharpe ratio: -0.006986778714377442\n",
      "step 3, loss: 38.06489944458008, sharpe ratio: 0.03195295664585856\n",
      "step 4, loss: 38.04113006591797, sharpe ratio: 0.02076768337803085\n",
      "step 5, loss: 38.012916564941406, sharpe ratio: -0.007122814249354634\n",
      "step 6, loss: 37.9727783203125, sharpe ratio: -0.054238702490003246\n",
      "step 7, loss: 38.015132904052734, sharpe ratio: 0.08285575585230259\n",
      "step 8, loss: 37.94599914550781, sharpe ratio: -0.04213009186973906\n",
      "step 9, loss: 37.94036102294922, sharpe ratio: -0.04601203168853836\n",
      "step 10, loss: 37.933433532714844, sharpe ratio: -0.015086167006925801\n",
      "step 11, loss: 37.91328430175781, sharpe ratio: -0.03445414335262527\n",
      "step 12, loss: 37.887794494628906, sharpe ratio: -0.062231264508084666\n",
      "validation sharpe ratio: 0.013565206497167479\n",
      "epoch 2: loss: 38.00002670288086, learning rate: 9.999998292187229e-05\n",
      "epoch 3...\n",
      "step 0, loss: 37.855377197265625, sharpe ratio: -0.07893896917710404\n",
      "step 1, loss: 37.88377380371094, sharpe ratio: -0.015775128555360382\n",
      "step 2, loss: 37.90251922607422, sharpe ratio: 0.041464530661334494\n",
      "step 3, loss: 37.86479187011719, sharpe ratio: 0.02820453934508389\n",
      "step 4, loss: 37.815032958984375, sharpe ratio: -0.0448986991419628\n",
      "step 5, loss: 37.859405517578125, sharpe ratio: 0.03818048083675423\n",
      "step 6, loss: 37.84724807739258, sharpe ratio: 0.0178291037861624\n",
      "step 7, loss: 37.774375915527344, sharpe ratio: -0.06399316383450003\n",
      "step 8, loss: 37.8109016418457, sharpe ratio: 0.019876168072060843\n",
      "step 9, loss: 37.81038284301758, sharpe ratio: 0.04807875104801837\n",
      "step 10, loss: 37.81531524658203, sharpe ratio: 0.045655200227690894\n",
      "step 11, loss: 37.74369812011719, sharpe ratio: -0.02415803726203614\n",
      "step 12, loss: 37.77043533325195, sharpe ratio: 0.05062314321702878\n",
      "validation sharpe ratio: 0.02392381838539758\n",
      "epoch 3: loss: 37.82717514038086, learning rate: 9.999998292187229e-05\n",
      "epoch 4...\n",
      "step 0, loss: 37.717002868652344, sharpe ratio: -0.028908262560999057\n",
      "step 1, loss: 37.725311279296875, sharpe ratio: 0.026971107046165864\n",
      "step 2, loss: 37.68321228027344, sharpe ratio: -0.016692476398370286\n",
      "step 3, loss: 37.685279846191406, sharpe ratio: 0.018331942178232096\n",
      "step 4, loss: 37.693199157714844, sharpe ratio: 0.052235370472460796\n",
      "step 5, loss: 37.644290924072266, sharpe ratio: -0.01854482651967817\n",
      "step 6, loss: 37.648193359375, sharpe ratio: 0.001953532703195376\n",
      "step 7, loss: 37.66373062133789, sharpe ratio: 0.06210274625328209\n",
      "step 8, loss: 37.62153625488281, sharpe ratio: -0.0012626447601095884\n",
      "step 9, loss: 37.60116195678711, sharpe ratio: -0.02400720583266363\n",
      "step 10, loss: 37.5621337890625, sharpe ratio: -0.06179885289589203\n",
      "step 11, loss: 37.5494499206543, sharpe ratio: -0.06249208392279221\n",
      "step 12, loss: 37.60504913330078, sharpe ratio: 0.06728323332019297\n",
      "validation sharpe ratio: 0.03635196713420935\n",
      "epoch 4: loss: 37.646114349365234, learning rate: 9.999998292187229e-05\n",
      "epoch 5...\n",
      "step 0, loss: 37.50415802001953, sharpe ratio: -0.10695698078435907\n",
      "step 1, loss: 37.566978454589844, sharpe ratio: 0.05358897003770034\n",
      "step 2, loss: 37.44514083862305, sharpe ratio: -0.13148725012651902\n",
      "step 3, loss: 37.57541275024414, sharpe ratio: 0.12107451396183849\n",
      "step 4, loss: 37.494300842285156, sharpe ratio: 0.01001316176589303\n",
      "step 5, loss: 37.551849365234375, sharpe ratio: 0.11108958728556165\n",
      "step 6, loss: 37.50690841674805, sharpe ratio: 0.062288518693702666\n",
      "step 7, loss: 37.439613342285156, sharpe ratio: -0.036996605900507844\n",
      "step 8, loss: 37.45698165893555, sharpe ratio: 0.023343886763601725\n",
      "step 9, loss: 37.40766525268555, sharpe ratio: -0.04721554977568397\n",
      "step 10, loss: 37.42000198364258, sharpe ratio: -0.014294296225970156\n",
      "step 11, loss: 37.39105987548828, sharpe ratio: -0.024790803073358618\n",
      "step 12, loss: 37.41301345825195, sharpe ratio: 0.029892272809562775\n",
      "validation sharpe ratio: 0.05634392693223919\n",
      "epoch 5: loss: 37.474849700927734, learning rate: 9.999998292187229e-05\n",
      "epoch 6...\n",
      "step 0, loss: 37.369014739990234, sharpe ratio: -0.005744391007046941\n",
      "step 1, loss: 37.388671875, sharpe ratio: 0.03539832715498051\n",
      "step 2, loss: 37.363792419433594, sharpe ratio: 0.025585894815747245\n",
      "step 3, loss: 37.35641860961914, sharpe ratio: 0.026974661238624117\n",
      "step 4, loss: 37.32740783691406, sharpe ratio: -0.022955762283735724\n",
      "step 5, loss: 37.315860748291016, sharpe ratio: 0.0034825943998039732\n",
      "step 6, loss: 37.36049270629883, sharpe ratio: 0.1007613912806955\n",
      "step 7, loss: 37.270206451416016, sharpe ratio: -0.04888371190470713\n",
      "step 8, loss: 37.22853469848633, sharpe ratio: -0.06743216855014977\n",
      "step 9, loss: 37.26713562011719, sharpe ratio: 0.010981296530025043\n",
      "step 10, loss: 37.26604080200195, sharpe ratio: 0.03221746678039572\n",
      "step 11, loss: 37.23657989501953, sharpe ratio: 0.003012352067427098\n",
      "step 12, loss: 37.213523864746094, sharpe ratio: -0.025179773719059374\n",
      "validation sharpe ratio: 0.046962371665461106\n",
      "epoch 6: loss: 37.304901123046875, learning rate: 9.999998292187229e-05\n",
      "epoch 7...\n",
      "step 0, loss: 37.21438217163086, sharpe ratio: 0.00972734967979345\n",
      "step 1, loss: 37.183467864990234, sharpe ratio: -0.023192238885485772\n",
      "step 2, loss: 37.20513153076172, sharpe ratio: 0.05519924037863959\n",
      "step 3, loss: 37.149959564208984, sharpe ratio: -0.02605420929875076\n",
      "step 4, loss: 37.13253402709961, sharpe ratio: -0.029427617097743903\n",
      "step 5, loss: 37.118080139160156, sharpe ratio: -0.05209799597085399\n",
      "step 6, loss: 37.13155746459961, sharpe ratio: 0.01465094525682792\n",
      "step 7, loss: 37.11900329589844, sharpe ratio: 0.018677154376589362\n",
      "step 8, loss: 37.11861038208008, sharpe ratio: 0.04895778453996829\n",
      "step 9, loss: 37.04258728027344, sharpe ratio: -0.06975616819549542\n",
      "step 10, loss: 37.06920623779297, sharpe ratio: -0.011786698028038237\n",
      "step 11, loss: 37.07936096191406, sharpe ratio: 0.046514958936936746\n",
      "step 12, loss: 37.08512496948242, sharpe ratio: 0.06011425502374901\n",
      "validation sharpe ratio: 0.046459081817700136\n",
      "epoch 7: loss: 37.12684631347656, learning rate: 9.999998292187229e-05\n",
      "epoch 8...\n",
      "step 0, loss: 37.0256462097168, sharpe ratio: 0.0014532751861382406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss: 37.00373077392578, sharpe ratio: -0.021560626599606628\n",
      "step 2, loss: 36.987457275390625, sharpe ratio: -0.05063544673697101\n",
      "step 3, loss: 36.95005798339844, sharpe ratio: -0.0698784449546743\n",
      "step 4, loss: 37.028133392333984, sharpe ratio: 0.07066955620600776\n",
      "step 5, loss: 37.009071350097656, sharpe ratio: 0.06805727249703225\n",
      "step 6, loss: 36.94791793823242, sharpe ratio: -0.020624582807326063\n",
      "step 7, loss: 36.90911102294922, sharpe ratio: -0.05696036517066112\n",
      "step 8, loss: 36.92042541503906, sharpe ratio: -0.016906152580025584\n",
      "step 9, loss: 36.944942474365234, sharpe ratio: 0.06939143662525249\n",
      "step 10, loss: 36.911216735839844, sharpe ratio: 0.020723103542859254\n",
      "step 11, loss: 36.87653732299805, sharpe ratio: -0.02590210542786714\n",
      "step 12, loss: 36.866554260253906, sharpe ratio: -0.011065157757128158\n",
      "validation sharpe ratio: 0.04696218230157179\n",
      "epoch 8: loss: 36.952369689941406, learning rate: 9.999998292187229e-05\n",
      "epoch 9...\n",
      "step 0, loss: 36.85558319091797, sharpe ratio: -0.007720796423367107\n",
      "step 1, loss: 36.851871490478516, sharpe ratio: 0.003280919006778753\n",
      "step 2, loss: 36.86296081542969, sharpe ratio: 0.050529356758390834\n",
      "step 3, loss: 36.835357666015625, sharpe ratio: 0.018766282737399772\n",
      "step 4, loss: 36.83051300048828, sharpe ratio: 0.039178262913362155\n",
      "step 5, loss: 36.75434875488281, sharpe ratio: -0.055337696865225494\n",
      "step 6, loss: 36.790199279785156, sharpe ratio: 0.032421365680645575\n",
      "step 7, loss: 36.77352523803711, sharpe ratio: 0.01632961952396053\n",
      "step 8, loss: 36.73164749145508, sharpe ratio: -0.032112162781872504\n",
      "step 9, loss: 36.774288177490234, sharpe ratio: 0.053702578382855505\n",
      "step 10, loss: 36.72182846069336, sharpe ratio: -0.019296305990226456\n",
      "step 11, loss: 36.65681076049805, sharpe ratio: -0.08674219104901118\n",
      "step 12, loss: 36.63924789428711, sharpe ratio: -0.12066464134496996\n",
      "validation sharpe ratio: 0.040760047270074616\n",
      "epoch 9: loss: 36.775245666503906, learning rate: 9.999998292187229e-05\n",
      "epoch 10...\n",
      "step 0, loss: 36.6611213684082, sharpe ratio: -0.05457188830492364\n",
      "step 1, loss: 36.673126220703125, sharpe ratio: -0.002982784810679786\n",
      "step 2, loss: 36.65874099731445, sharpe ratio: 0.006315379328954574\n",
      "step 3, loss: 36.646751403808594, sharpe ratio: -0.009544471136536673\n",
      "step 4, loss: 36.65101623535156, sharpe ratio: 0.019565675222254688\n",
      "step 5, loss: 36.63734436035156, sharpe ratio: 0.02720027325264336\n",
      "step 6, loss: 36.584228515625, sharpe ratio: -0.031465784959332506\n",
      "step 7, loss: 36.512115478515625, sharpe ratio: -0.14226670043153788\n",
      "step 8, loss: 36.60425567626953, sharpe ratio: 0.04357678200233481\n",
      "step 9, loss: 36.605018615722656, sharpe ratio: 0.10562237957694132\n",
      "step 10, loss: 36.543338775634766, sharpe ratio: -0.02069054378387196\n",
      "step 11, loss: 36.52320861816406, sharpe ratio: -0.03499571792870969\n",
      "step 12, loss: 36.546539306640625, sharpe ratio: 0.028314645038690002\n",
      "validation sharpe ratio: 0.05353889344966125\n",
      "epoch 10: loss: 36.603599548339844, learning rate: 9.999998292187229e-05\n",
      "epoch 11...\n",
      "step 0, loss: 36.48871994018555, sharpe ratio: -0.03655927970413492\n",
      "step 1, loss: 36.508750915527344, sharpe ratio: 0.020283028730896942\n",
      "step 2, loss: 36.48148727416992, sharpe ratio: -0.013917289129603324\n",
      "step 3, loss: 36.45891571044922, sharpe ratio: -0.014022858231165954\n",
      "step 4, loss: 36.46809768676758, sharpe ratio: -0.0016012403319485419\n",
      "step 5, loss: 36.44808578491211, sharpe ratio: 8.077174152499797e-05\n",
      "step 6, loss: 36.48279571533203, sharpe ratio: 0.10028938909319712\n",
      "step 7, loss: 36.39716339111328, sharpe ratio: -0.03780830228398254\n",
      "step 8, loss: 36.36916732788086, sharpe ratio: -0.06353767635072502\n",
      "step 9, loss: 36.3641471862793, sharpe ratio: -0.0495731844806728\n",
      "step 10, loss: 36.388126373291016, sharpe ratio: 0.02957536930228221\n",
      "step 11, loss: 36.3956184387207, sharpe ratio: 0.0439175484320731\n",
      "step 12, loss: 36.32329559326172, sharpe ratio: -0.06202228541474021\n",
      "validation sharpe ratio: 0.0633190710949232\n",
      "epoch 11: loss: 36.42879867553711, learning rate: 9.999998292187229e-05\n",
      "epoch 12...\n",
      "step 0, loss: 36.41059875488281, sharpe ratio: 0.12588469346022976\n",
      "step 1, loss: 36.35955810546875, sharpe ratio: 0.04889912797108655\n",
      "step 2, loss: 36.27937698364258, sharpe ratio: -0.06273137021788489\n",
      "step 3, loss: 36.29596710205078, sharpe ratio: -0.014100451325570675\n",
      "step 4, loss: 36.24817657470703, sharpe ratio: -0.055406149794296505\n",
      "step 5, loss: 36.29656219482422, sharpe ratio: 0.028436053552308473\n",
      "step 6, loss: 36.30021286010742, sharpe ratio: 0.05668643986441009\n",
      "step 7, loss: 36.23698425292969, sharpe ratio: -0.01914133731788878\n",
      "step 8, loss: 36.2486686706543, sharpe ratio: 0.014832070862398586\n",
      "step 9, loss: 36.18563461303711, sharpe ratio: -0.08241655272216812\n",
      "step 10, loss: 36.20030975341797, sharpe ratio: -0.019537330763099048\n",
      "step 11, loss: 36.24409103393555, sharpe ratio: 0.10202194767165695\n",
      "step 12, loss: 36.18950653076172, sharpe ratio: 0.01888172654446816\n",
      "validation sharpe ratio: 0.04448827189640307\n",
      "epoch 12: loss: 36.26889419555664, learning rate: 9.999998292187229e-05\n",
      "epoch 13...\n",
      "step 0, loss: 36.180049896240234, sharpe ratio: 0.009240716299870598\n",
      "step 1, loss: 36.12361145019531, sharpe ratio: -0.06898237422546706\n",
      "step 2, loss: 36.14990234375, sharpe ratio: -0.005442078659886334\n",
      "step 3, loss: 36.16590118408203, sharpe ratio: 0.07855436656579672\n",
      "step 4, loss: 36.07234191894531, sharpe ratio: -0.08946587644865109\n",
      "step 5, loss: 36.15840530395508, sharpe ratio: 0.07797914848214499\n",
      "step 6, loss: 36.06594467163086, sharpe ratio: -0.050865484922670526\n",
      "step 7, loss: 36.06151580810547, sharpe ratio: -0.062404635347969736\n",
      "step 8, loss: 36.06745147705078, sharpe ratio: -0.0005459584924235455\n",
      "step 9, loss: 36.05205535888672, sharpe ratio: 0.0022740737546506073\n",
      "step 10, loss: 36.079345703125, sharpe ratio: 0.06336767852320793\n",
      "step 11, loss: 35.98558044433594, sharpe ratio: -0.07366345043583077\n",
      "step 12, loss: 36.03988265991211, sharpe ratio: 0.06008520532571321\n",
      "validation sharpe ratio: 0.04726838530197239\n",
      "epoch 13: loss: 36.09246063232422, learning rate: 9.999998292187229e-05\n",
      "epoch 14...\n",
      "step 0, loss: 36.010528564453125, sharpe ratio: 0.00021503318320568836\n",
      "step 1, loss: 35.985565185546875, sharpe ratio: -0.011638651728982588\n",
      "step 2, loss: 35.990421295166016, sharpe ratio: 0.022131189406981028\n",
      "step 3, loss: 35.98573684692383, sharpe ratio: 0.04706433306554693\n",
      "step 4, loss: 35.91877746582031, sharpe ratio: -0.06871451874520874\n",
      "step 5, loss: 35.96024703979492, sharpe ratio: 0.028773184211685995\n",
      "step 6, loss: 35.87217330932617, sharpe ratio: -0.10390773358145113\n",
      "step 7, loss: 35.88832473754883, sharpe ratio: -0.028259846893948157\n",
      "step 8, loss: 35.914817810058594, sharpe ratio: 0.05134212071653672\n",
      "step 9, loss: 35.9363899230957, sharpe ratio: 0.08290702417239523\n",
      "step 10, loss: 35.91720962524414, sharpe ratio: 0.08218353748686143\n",
      "step 11, loss: 35.82024383544922, sharpe ratio: -0.06179785393062989\n",
      "step 12, loss: 35.865543365478516, sharpe ratio: 0.03758580357907269\n",
      "validation sharpe ratio: 0.04713832183748246\n",
      "epoch 14: loss: 35.92815399169922, learning rate: 9.999998292187229e-05\n",
      "epoch 15...\n",
      "step 0, loss: 35.862552642822266, sharpe ratio: 0.06664572308496101\n",
      "step 1, loss: 35.82190704345703, sharpe ratio: 0.0029532945070847695\n",
      "step 2, loss: 35.80415344238281, sharpe ratio: -0.01526284516316846\n",
      "step 3, loss: 35.779563903808594, sharpe ratio: -0.029205999651578166\n",
      "step 4, loss: 35.782676696777344, sharpe ratio: 0.0035886763758261643\n",
      "step 5, loss: 35.7835578918457, sharpe ratio: 0.02927910035840463\n",
      "step 6, loss: 35.77515411376953, sharpe ratio: 0.033400827549835296\n",
      "step 7, loss: 35.67110824584961, sharpe ratio: -0.1121677477179892\n",
      "step 8, loss: 35.735897064208984, sharpe ratio: 0.0244539139375509\n",
      "step 9, loss: 35.69156265258789, sharpe ratio: -0.05416550136867872\n",
      "step 10, loss: 35.699668884277344, sharpe ratio: -0.0008572715090457922\n",
      "step 11, loss: 35.73292541503906, sharpe ratio: 0.08389739229814683\n",
      "step 12, loss: 35.66617202758789, sharpe ratio: -0.008462332459905342\n",
      "validation sharpe ratio: 0.04623229680747259\n",
      "epoch 15: loss: 35.75437927246094, learning rate: 9.999998292187229e-05\n",
      "epoch 16...\n",
      "step 0, loss: 35.666786193847656, sharpe ratio: 0.004014864422203321\n",
      "step 1, loss: 35.630523681640625, sharpe ratio: -0.04566905243300118\n",
      "step 2, loss: 35.680931091308594, sharpe ratio: 0.05411169184749256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3, loss: 35.621829986572266, sharpe ratio: -0.005061245603752483\n",
      "step 4, loss: 35.64020538330078, sharpe ratio: 0.05347963212875942\n",
      "step 5, loss: 35.61172866821289, sharpe ratio: 0.03453124604755357\n",
      "step 6, loss: 35.61831283569336, sharpe ratio: 0.057896899678955685\n",
      "step 7, loss: 35.60698318481445, sharpe ratio: 0.06718028495243888\n",
      "step 8, loss: 35.50523376464844, sharpe ratio: -0.08985851383454121\n",
      "step 9, loss: 35.56990432739258, sharpe ratio: 0.03037505237953892\n",
      "step 10, loss: 35.52898025512695, sharpe ratio: -0.008291196009408672\n",
      "step 11, loss: 35.53547286987305, sharpe ratio: 0.018204582026634622\n",
      "step 12, loss: 35.52581787109375, sharpe ratio: 0.03537442539715914\n",
      "validation sharpe ratio: 0.04900194193930532\n",
      "epoch 16: loss: 35.5955924987793, learning rate: 9.999998292187229e-05\n",
      "epoch 17...\n",
      "step 0, loss: 35.47085952758789, sharpe ratio: -0.05357780443070622\n",
      "step 1, loss: 35.5041618347168, sharpe ratio: 0.03782163334687446\n",
      "step 2, loss: 35.448036193847656, sharpe ratio: -0.04217457836171515\n",
      "step 3, loss: 35.47127914428711, sharpe ratio: 0.01661853800398346\n",
      "step 4, loss: 35.45649719238281, sharpe ratio: 0.021084454384299532\n",
      "step 5, loss: 35.441802978515625, sharpe ratio: 0.02808658937214455\n",
      "step 6, loss: 35.41476821899414, sharpe ratio: 0.0005533974034220903\n",
      "step 7, loss: 35.439476013183594, sharpe ratio: 0.05831802963412939\n",
      "step 8, loss: 35.42747116088867, sharpe ratio: 0.056824196006632285\n",
      "step 9, loss: 35.405555725097656, sharpe ratio: 0.05717282625003556\n",
      "step 10, loss: 35.36152267456055, sharpe ratio: 0.0017572190923796221\n",
      "step 11, loss: 35.35327911376953, sharpe ratio: -0.007501175417974636\n",
      "step 12, loss: 35.312198638916016, sharpe ratio: -0.04912732465197251\n",
      "validation sharpe ratio: 0.059708635059572884\n",
      "epoch 17: loss: 35.423606872558594, learning rate: 9.999998292187229e-05\n",
      "epoch 18...\n",
      "step 0, loss: 35.3177604675293, sharpe ratio: -0.01802676882179002\n",
      "step 1, loss: 35.308448791503906, sharpe ratio: 0.010471765091892349\n",
      "step 2, loss: 35.28839111328125, sharpe ratio: -0.029689387479018222\n",
      "step 3, loss: 35.27872085571289, sharpe ratio: -0.022641737826912485\n",
      "step 4, loss: 35.28763961791992, sharpe ratio: 0.0007973544506084005\n",
      "step 5, loss: 35.23851013183594, sharpe ratio: -0.03936969812616656\n",
      "step 6, loss: 35.21523666381836, sharpe ratio: -0.04349917159014074\n",
      "step 7, loss: 35.22156524658203, sharpe ratio: -0.001171236541522229\n",
      "step 8, loss: 35.2569580078125, sharpe ratio: 0.047182640925205147\n",
      "step 9, loss: 35.256507873535156, sharpe ratio: 0.08617156425053962\n",
      "step 10, loss: 35.20244216918945, sharpe ratio: 0.005318263330785861\n",
      "step 11, loss: 35.147735595703125, sharpe ratio: -0.05695484517451631\n",
      "step 12, loss: 35.17643356323242, sharpe ratio: 0.022718309720560218\n",
      "validation sharpe ratio: 0.059708635059572884\n",
      "epoch 18: loss: 35.245872497558594, learning rate: 9.999998292187229e-05\n",
      "epoch 19...\n",
      "step 0, loss: 35.182430267333984, sharpe ratio: 0.07570513545451539\n",
      "step 1, loss: 35.183162689208984, sharpe ratio: 0.06151846234916819\n",
      "step 2, loss: 35.152095794677734, sharpe ratio: 0.02745604161903057\n",
      "step 3, loss: 35.141502380371094, sharpe ratio: 0.027313609408373345\n",
      "step 4, loss: 35.10569381713867, sharpe ratio: 9.907992721681913e-05\n",
      "step 5, loss: 35.12945556640625, sharpe ratio: 0.055836580084721976\n",
      "step 6, loss: 35.09965515136719, sharpe ratio: 0.020409341460881768\n",
      "step 7, loss: 35.056480407714844, sharpe ratio: -0.015829691955281038\n",
      "step 8, loss: 35.04206085205078, sharpe ratio: -0.01968534862683606\n",
      "step 9, loss: 35.06226348876953, sharpe ratio: 0.06522805959033275\n",
      "step 10, loss: 34.987098693847656, sharpe ratio: -0.0885399757197474\n",
      "step 11, loss: 35.03546905517578, sharpe ratio: 0.034342074352562084\n",
      "step 12, loss: 35.0483283996582, sharpe ratio: 0.07292863918927336\n",
      "validation sharpe ratio: 0.06018410705679607\n",
      "epoch 19: loss: 35.09428405761719, learning rate: 9.999998292187229e-05\n",
      "epoch 20...\n",
      "step 0, loss: 34.979557037353516, sharpe ratio: -0.02225781209620037\n",
      "step 1, loss: 35.010311126708984, sharpe ratio: 0.04321127960666534\n",
      "step 2, loss: 35.00017166137695, sharpe ratio: 0.07307498708960761\n",
      "step 3, loss: 34.93903732299805, sharpe ratio: -0.02105825724805129\n",
      "step 4, loss: 34.980167388916016, sharpe ratio: 0.07289129380552492\n",
      "step 5, loss: 34.91402053833008, sharpe ratio: -0.025391837314444894\n",
      "step 6, loss: 34.882835388183594, sharpe ratio: -0.05249040953616804\n",
      "step 7, loss: 34.925655364990234, sharpe ratio: 0.053516138624580115\n",
      "step 8, loss: 34.881629943847656, sharpe ratio: -0.01856396437246894\n",
      "step 9, loss: 34.90264892578125, sharpe ratio: 0.058554099455569275\n",
      "step 10, loss: 34.887203216552734, sharpe ratio: 0.045105891811641416\n",
      "step 11, loss: 34.85184097290039, sharpe ratio: 0.0009057003590341034\n",
      "step 12, loss: 34.86241912841797, sharpe ratio: 0.032101776044348045\n",
      "validation sharpe ratio: 0.059857211054532046\n",
      "epoch 20: loss: 34.92442321777344, learning rate: 9.999998292187229e-05\n",
      "epoch 21...\n",
      "step 0, loss: 34.843257904052734, sharpe ratio: 0.01845566453486113\n",
      "step 1, loss: 34.7810173034668, sharpe ratio: -0.07894576038207\n",
      "step 2, loss: 34.8095588684082, sharpe ratio: 0.010132601682914615\n",
      "step 3, loss: 34.734989166259766, sharpe ratio: -0.08346467961091719\n",
      "step 4, loss: 34.82370376586914, sharpe ratio: 0.10352687499370355\n",
      "step 5, loss: 34.74756622314453, sharpe ratio: -0.022779349948880097\n",
      "step 6, loss: 34.77915954589844, sharpe ratio: 0.051217125289957297\n",
      "step 7, loss: 34.793575286865234, sharpe ratio: 0.09113739319990943\n",
      "step 8, loss: 34.76258087158203, sharpe ratio: 0.05140106471737807\n",
      "step 9, loss: 34.717647552490234, sharpe ratio: 0.013491287850008258\n",
      "step 10, loss: 34.651302337646484, sharpe ratio: -0.08621666301350779\n",
      "step 11, loss: 34.6583251953125, sharpe ratio: -0.030401200404289976\n",
      "step 12, loss: 34.65239715576172, sharpe ratio: -0.046879858078481045\n",
      "validation sharpe ratio: 0.054868276315891994\n",
      "epoch 21: loss: 34.750389099121094, learning rate: 9.999998292187229e-05\n",
      "epoch 22...\n",
      "step 0, loss: 34.70554733276367, sharpe ratio: 0.06500295545291687\n",
      "step 1, loss: 34.69291305541992, sharpe ratio: 0.06730638024782816\n",
      "step 2, loss: 34.675045013427734, sharpe ratio: 0.05626564065250257\n",
      "step 3, loss: 34.60279083251953, sharpe ratio: -0.07841145037722187\n",
      "step 4, loss: 34.59830856323242, sharpe ratio: -0.02885380699795475\n",
      "step 5, loss: 34.526573181152344, sharpe ratio: -0.12288515661493057\n",
      "step 6, loss: 34.573238372802734, sharpe ratio: -0.028717660094668876\n",
      "step 7, loss: 34.59415054321289, sharpe ratio: 0.03491096770794619\n",
      "step 8, loss: 34.554222106933594, sharpe ratio: 0.001974572246469542\n",
      "step 9, loss: 34.54817199707031, sharpe ratio: 0.00585143851575398\n",
      "step 10, loss: 34.54093933105469, sharpe ratio: 0.0054172092194186925\n",
      "step 11, loss: 34.52925109863281, sharpe ratio: 0.010963989849610466\n",
      "step 12, loss: 34.491050720214844, sharpe ratio: -0.04378939715222146\n",
      "validation sharpe ratio: 0.05440270545569707\n",
      "epoch 22: loss: 34.587093353271484, learning rate: 9.999998292187229e-05\n",
      "epoch 23...\n",
      "step 0, loss: 34.46002197265625, sharpe ratio: -0.07759891300870506\n",
      "step 1, loss: 34.48529052734375, sharpe ratio: -0.008923934425189099\n",
      "step 2, loss: 34.475608825683594, sharpe ratio: 0.012111296488116987\n",
      "step 3, loss: 34.47461700439453, sharpe ratio: 0.010786137448093185\n",
      "step 4, loss: 34.45215606689453, sharpe ratio: 0.030623927152982393\n",
      "step 5, loss: 34.41094207763672, sharpe ratio: -0.02954788506842241\n",
      "step 6, loss: 34.4379768371582, sharpe ratio: 0.032660003695866746\n",
      "step 7, loss: 34.41780471801758, sharpe ratio: 0.017880038749236146\n",
      "step 8, loss: 34.41010665893555, sharpe ratio: 0.02278820816362767\n",
      "step 9, loss: 34.40861892700195, sharpe ratio: 0.044359064797128096\n",
      "step 10, loss: 34.37736511230469, sharpe ratio: 0.00807195191661942\n",
      "step 11, loss: 34.35194396972656, sharpe ratio: -0.010430280217351536\n",
      "step 12, loss: 34.303550720214844, sharpe ratio: -0.06407507128812077\n",
      "validation sharpe ratio: 0.054826338177350685\n",
      "epoch 23: loss: 34.42045974731445, learning rate: 9.999998292187229e-05\n",
      "epoch 24...\n",
      "step 0, loss: 34.31051254272461, sharpe ratio: -0.04096170360386906\n",
      "step 1, loss: 34.28468322753906, sharpe ratio: -0.03863867679533265\n",
      "step 2, loss: 34.24808120727539, sharpe ratio: -0.10586618109920207\n",
      "step 3, loss: 34.318111419677734, sharpe ratio: 0.056002388972636215\n",
      "step 4, loss: 34.275054931640625, sharpe ratio: -0.007537874279875433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5, loss: 34.21833419799805, sharpe ratio: -0.08813006983707558\n",
      "step 6, loss: 34.288536071777344, sharpe ratio: 0.06680928463645092\n",
      "step 7, loss: 34.27670669555664, sharpe ratio: 0.06545221922093072\n",
      "step 8, loss: 34.259605407714844, sharpe ratio: 0.05356963529617764\n",
      "step 9, loss: 34.236995697021484, sharpe ratio: 0.019687429326066878\n",
      "step 10, loss: 34.205814361572266, sharpe ratio: -0.003299211596672888\n",
      "step 11, loss: 34.216949462890625, sharpe ratio: 0.033733901218236786\n",
      "step 12, loss: 34.173954010009766, sharpe ratio: -0.021628176516555363\n",
      "validation sharpe ratio: 0.057035179710935774\n",
      "epoch 24: loss: 34.25487518310547, learning rate: 9.999998292187229e-05\n",
      "epoch 25...\n",
      "step 0, loss: 34.16582489013672, sharpe ratio: -0.015150090330588966\n",
      "step 1, loss: 34.20241928100586, sharpe ratio: 0.07406919797600313\n",
      "step 2, loss: 34.108863830566406, sharpe ratio: -0.05758112041893708\n",
      "step 3, loss: 34.12947082519531, sharpe ratio: 0.005373351863217321\n",
      "step 4, loss: 34.172122955322266, sharpe ratio: 0.1141912960521592\n",
      "step 5, loss: 34.116477966308594, sharpe ratio: 0.019450726730735477\n",
      "step 6, loss: 34.074161529541016, sharpe ratio: -0.03426934272421566\n",
      "step 7, loss: 34.08797836303711, sharpe ratio: 0.007851602973451752\n",
      "step 8, loss: 34.07456970214844, sharpe ratio: 0.005654675401842196\n",
      "step 9, loss: 34.05651092529297, sharpe ratio: -0.005445441477668789\n",
      "step 10, loss: 34.0362548828125, sharpe ratio: -0.019001907503661587\n",
      "step 11, loss: 34.04466247558594, sharpe ratio: 0.023764105458663486\n",
      "step 12, loss: 34.01933670043945, sharpe ratio: 0.012575022615799712\n",
      "validation sharpe ratio: 0.06265013240800993\n",
      "epoch 25: loss: 34.09912872314453, learning rate: 9.999998292187229e-05\n",
      "epoch 26...\n",
      "step 0, loss: 34.00520706176758, sharpe ratio: -0.005598168139401536\n",
      "step 1, loss: 33.99967575073242, sharpe ratio: 0.009940986680358619\n",
      "step 2, loss: 33.9466552734375, sharpe ratio: -0.06017487754556173\n",
      "step 3, loss: 33.99396514892578, sharpe ratio: 0.034589235923358275\n",
      "step 4, loss: 33.98828887939453, sharpe ratio: 0.05953020722977588\n",
      "step 5, loss: 33.888301849365234, sharpe ratio: -0.08200596122637249\n",
      "step 6, loss: 33.91490936279297, sharpe ratio: -0.03247560851274947\n",
      "step 7, loss: 33.936676025390625, sharpe ratio: 0.03337152861736161\n",
      "step 8, loss: 33.91282653808594, sharpe ratio: -0.00373091214486769\n",
      "step 9, loss: 33.89931106567383, sharpe ratio: 0.021086151594135828\n",
      "step 10, loss: 33.89929962158203, sharpe ratio: 0.032677622586856477\n",
      "step 11, loss: 33.900115966796875, sharpe ratio: 0.059554063691434116\n",
      "step 12, loss: 33.84769821166992, sharpe ratio: -0.018397236588494133\n",
      "validation sharpe ratio: 0.06216888527670178\n",
      "epoch 26: loss: 33.93330001831055, learning rate: 9.999998292187229e-05\n",
      "epoch 27...\n",
      "step 0, loss: 33.8097038269043, sharpe ratio: -0.07463794531603773\n",
      "step 1, loss: 33.82948684692383, sharpe ratio: -0.005926223996431413\n",
      "step 2, loss: 33.80570602416992, sharpe ratio: -0.02405745626621824\n",
      "step 3, loss: 33.768157958984375, sharpe ratio: -0.06487384869490426\n",
      "step 4, loss: 33.734317779541016, sharpe ratio: -0.11112443734454007\n",
      "step 5, loss: 33.80459213256836, sharpe ratio: 0.0626204667648899\n",
      "step 6, loss: 33.756561279296875, sharpe ratio: -0.027722227335639307\n",
      "step 7, loss: 33.71450424194336, sharpe ratio: -0.0909702903207516\n",
      "step 8, loss: 33.75159454345703, sharpe ratio: -0.019288392996882255\n",
      "step 9, loss: 33.71800994873047, sharpe ratio: -0.014013728328401998\n",
      "step 10, loss: 33.75904083251953, sharpe ratio: 0.08564287856885217\n",
      "step 11, loss: 33.711891174316406, sharpe ratio: 0.014205004764492392\n",
      "step 12, loss: 33.69023895263672, sharpe ratio: 0.003820315968410648\n",
      "validation sharpe ratio: 0.06313542585275994\n",
      "epoch 27: loss: 33.75798416137695, learning rate: 9.999998292187229e-05\n",
      "epoch 28...\n",
      "step 0, loss: 33.66841506958008, sharpe ratio: -0.01872798344251473\n",
      "step 1, loss: 33.68780517578125, sharpe ratio: 0.026703336987140926\n",
      "step 2, loss: 33.66041946411133, sharpe ratio: 0.0011047396746928576\n",
      "step 3, loss: 33.6336555480957, sharpe ratio: -0.027625432481802756\n",
      "step 4, loss: 33.65363693237305, sharpe ratio: 0.03253214584109182\n",
      "step 5, loss: 33.66901779174805, sharpe ratio: 0.08629326279726299\n",
      "step 6, loss: 33.60296630859375, sharpe ratio: -0.013541873377639258\n",
      "step 7, loss: 33.585758209228516, sharpe ratio: -0.022292498968045485\n",
      "step 8, loss: 33.60341262817383, sharpe ratio: 0.0400811044782621\n",
      "step 9, loss: 33.5442008972168, sharpe ratio: -0.05163656736538697\n",
      "step 10, loss: 33.55744934082031, sharpe ratio: 0.012336111569190025\n",
      "step 11, loss: 33.555885314941406, sharpe ratio: 0.03128456024515572\n",
      "step 12, loss: 33.56464385986328, sharpe ratio: 0.07570946475807794\n",
      "validation sharpe ratio: 0.06176552227228331\n",
      "epoch 28: loss: 33.61440658569336, learning rate: 9.999998292187229e-05\n",
      "epoch 29...\n",
      "step 0, loss: 33.51993179321289, sharpe ratio: -0.005554753831499371\n",
      "step 1, loss: 33.504215240478516, sharpe ratio: -0.008051587766790706\n",
      "step 2, loss: 33.49734878540039, sharpe ratio: -0.003802115403561483\n",
      "step 3, loss: 33.507293701171875, sharpe ratio: 0.03949715128884843\n",
      "step 4, loss: 33.40457534790039, sharpe ratio: -0.1448369778306944\n",
      "step 5, loss: 33.47455596923828, sharpe ratio: 0.03022659995408712\n",
      "step 6, loss: 33.41004180908203, sharpe ratio: -0.06952147309875124\n",
      "step 7, loss: 33.471168518066406, sharpe ratio: 0.07055544394854216\n",
      "step 8, loss: 33.415679931640625, sharpe ratio: -0.02410823477686709\n",
      "step 9, loss: 33.3850212097168, sharpe ratio: -0.040005793467778994\n",
      "step 10, loss: 33.34846115112305, sharpe ratio: -0.09157473402385123\n",
      "step 11, loss: 33.41653823852539, sharpe ratio: 0.04528147193211339\n",
      "step 12, loss: 33.36170959472656, sharpe ratio: -0.01794202565522839\n",
      "validation sharpe ratio: 0.0568625295483642\n",
      "epoch 29: loss: 33.43973159790039, learning rate: 9.999998292187229e-05\n",
      "epoch 30...\n",
      "step 0, loss: 33.38760757446289, sharpe ratio: 0.04377808640037798\n",
      "step 1, loss: 33.35873794555664, sharpe ratio: 0.023195638329924956\n",
      "step 2, loss: 33.36967468261719, sharpe ratio: 0.050766125994967284\n",
      "step 3, loss: 33.28184127807617, sharpe ratio: -0.08087746866847834\n",
      "step 4, loss: 33.29084396362305, sharpe ratio: -0.03785075673290014\n",
      "step 5, loss: 33.30959701538086, sharpe ratio: 0.01301361763861941\n",
      "step 6, loss: 33.27759552001953, sharpe ratio: -0.007216894122919339\n",
      "step 7, loss: 33.28573989868164, sharpe ratio: 0.025880121892914117\n",
      "step 8, loss: 33.267189025878906, sharpe ratio: 0.0031127394027976466\n",
      "step 9, loss: 33.2099723815918, sharpe ratio: -0.06681027146640489\n",
      "step 10, loss: 33.22995376586914, sharpe ratio: -0.007013424675531406\n",
      "step 11, loss: 33.201576232910156, sharpe ratio: -0.04316626143604983\n",
      "step 12, loss: 33.227325439453125, sharpe ratio: 0.03166732135966783\n",
      "validation sharpe ratio: 0.043609719650907076\n",
      "epoch 30: loss: 33.28443145751953, learning rate: 9.999998292187229e-05\n",
      "epoch 31...\n",
      "step 0, loss: 33.17772674560547, sharpe ratio: -0.04293280831353728\n",
      "step 1, loss: 33.23080825805664, sharpe ratio: 0.07608103413046609\n",
      "step 2, loss: 33.17707061767578, sharpe ratio: -0.00583491382666515\n",
      "step 3, loss: 33.147605895996094, sharpe ratio: -0.04327225380463785\n",
      "step 4, loss: 33.15945816040039, sharpe ratio: 0.019756436500259243\n",
      "step 5, loss: 33.15141677856445, sharpe ratio: -0.004676555150133294\n",
      "step 6, loss: 33.10490798950195, sharpe ratio: -0.033894814603211434\n",
      "step 7, loss: 33.10234451293945, sharpe ratio: -0.01860470074727252\n",
      "step 8, loss: 33.09049987792969, sharpe ratio: -0.009582986064568796\n",
      "step 9, loss: 33.09159851074219, sharpe ratio: 0.004544576566164506\n",
      "step 10, loss: 33.063716888427734, sharpe ratio: -0.0330644298565093\n",
      "step 11, loss: 33.071022033691406, sharpe ratio: 0.004466262258829066\n",
      "step 12, loss: 33.02851486206055, sharpe ratio: -0.05732042326008648\n",
      "validation sharpe ratio: 0.0423171169974417\n",
      "epoch 31: loss: 33.122825622558594, learning rate: 9.999998292187229e-05\n",
      "epoch 32...\n",
      "step 0, loss: 33.043296813964844, sharpe ratio: 0.018799741759302983\n",
      "step 1, loss: 32.97810745239258, sharpe ratio: -0.08103101202136784\n",
      "step 2, loss: 33.036705017089844, sharpe ratio: 0.03842365924518696\n",
      "step 3, loss: 33.006412506103516, sharpe ratio: 0.0074089608580072295\n",
      "step 4, loss: 32.98735046386719, sharpe ratio: 0.003838039223152255\n",
      "step 5, loss: 33.0005989074707, sharpe ratio: 0.026276805306563605\n",
      "step 6, loss: 32.98651885986328, sharpe ratio: 0.034234253703304296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7, loss: 32.970924377441406, sharpe ratio: 0.02921860691820026\n",
      "step 8, loss: 32.936973571777344, sharpe ratio: -0.01142634815116961\n",
      "step 9, loss: 32.90687561035156, sharpe ratio: -0.0486586440822781\n",
      "step 10, loss: 32.90241622924805, sharpe ratio: -0.02629708364465755\n",
      "step 11, loss: 32.94782638549805, sharpe ratio: 0.06613664429679267\n",
      "step 12, loss: 32.857242584228516, sharpe ratio: -0.044518905563239314\n",
      "validation sharpe ratio: 0.04549943641494446\n",
      "epoch 32: loss: 32.96624755859375, learning rate: 9.999998292187229e-05\n",
      "epoch 33...\n",
      "step 0, loss: 32.90021896362305, sharpe ratio: 0.03618952024515355\n",
      "step 1, loss: 32.82585906982422, sharpe ratio: -0.061590633978215736\n",
      "step 2, loss: 32.834938049316406, sharpe ratio: -0.028574306235792647\n",
      "step 3, loss: 32.86281967163086, sharpe ratio: 0.0016212036625069548\n",
      "step 4, loss: 32.83393096923828, sharpe ratio: 0.0034796307149516526\n",
      "step 5, loss: 32.85321044921875, sharpe ratio: 0.051874697647027145\n",
      "step 6, loss: 32.795143127441406, sharpe ratio: -0.01890340172589612\n",
      "step 7, loss: 32.828025817871094, sharpe ratio: 0.04989657534558166\n",
      "step 8, loss: 32.85403823852539, sharpe ratio: 0.11295602638917276\n",
      "step 9, loss: 32.793434143066406, sharpe ratio: 0.03508427011054432\n",
      "step 10, loss: 32.787288665771484, sharpe ratio: 0.055822761790069744\n",
      "step 11, loss: 32.78706359863281, sharpe ratio: 0.09839407257080655\n",
      "step 12, loss: 32.72093200683594, sharpe ratio: -0.02107274164888038\n",
      "validation sharpe ratio: 0.05424597890755708\n",
      "epoch 33: loss: 32.82129669189453, learning rate: 9.999998292187229e-05\n",
      "epoch 34...\n",
      "step 0, loss: 32.706764221191406, sharpe ratio: -0.021940756622958683\n",
      "step 1, loss: 32.704856872558594, sharpe ratio: -0.01030929166223513\n",
      "step 2, loss: 32.654850006103516, sharpe ratio: -0.07776457770887091\n",
      "step 3, loss: 32.698768615722656, sharpe ratio: 0.021530866989061065\n",
      "step 4, loss: 32.703041076660156, sharpe ratio: 0.04808047764136334\n",
      "step 5, loss: 32.65657424926758, sharpe ratio: -0.01385783183315801\n",
      "step 6, loss: 32.636226654052734, sharpe ratio: -0.023893188119959358\n",
      "step 7, loss: 32.63826370239258, sharpe ratio: 0.0005738429679471527\n",
      "step 8, loss: 32.61715316772461, sharpe ratio: -0.02292621244259684\n",
      "step 9, loss: 32.65654754638672, sharpe ratio: 0.07261484014529648\n",
      "step 10, loss: 32.6472053527832, sharpe ratio: 0.0843747598906412\n",
      "step 11, loss: 32.628692626953125, sharpe ratio: 0.07263480517661906\n",
      "step 12, loss: 32.5500602722168, sharpe ratio: -0.06514681289223051\n",
      "validation sharpe ratio: 0.045867119207571624\n",
      "epoch 34: loss: 32.653770446777344, learning rate: 9.999998292187229e-05\n",
      "epoch 35...\n",
      "step 0, loss: 32.542057037353516, sharpe ratio: -0.03665840068523312\n",
      "step 1, loss: 32.5531005859375, sharpe ratio: -0.01101638206578873\n",
      "step 2, loss: 32.53944396972656, sharpe ratio: -0.0006382846177234551\n",
      "step 3, loss: 32.5422477722168, sharpe ratio: 0.021178684312415003\n",
      "step 4, loss: 32.51751708984375, sharpe ratio: 0.017978271708504842\n",
      "step 5, loss: 32.49980163574219, sharpe ratio: -0.005086138005076745\n",
      "step 6, loss: 32.4834098815918, sharpe ratio: -0.015848844200104\n",
      "step 7, loss: 32.50081253051758, sharpe ratio: 0.030587108827948364\n",
      "step 8, loss: 32.470458984375, sharpe ratio: 0.018639872576373877\n",
      "step 9, loss: 32.487709045410156, sharpe ratio: 0.056798237347112965\n",
      "step 10, loss: 32.43911361694336, sharpe ratio: -0.015291659227619664\n",
      "step 11, loss: 32.44953536987305, sharpe ratio: 0.023887230540512214\n",
      "step 12, loss: 32.39790725708008, sharpe ratio: -0.06091634988556969\n",
      "validation sharpe ratio: 0.05303856896617806\n",
      "epoch 35: loss: 32.49408721923828, learning rate: 9.999998292187229e-05\n",
      "epoch 36...\n",
      "step 0, loss: 32.471702575683594, sharpe ratio: 0.10115991357429996\n",
      "step 1, loss: 32.38077926635742, sharpe ratio: -0.018101571896042903\n",
      "step 2, loss: 32.40378952026367, sharpe ratio: 0.023903775334081778\n",
      "step 3, loss: 32.37116241455078, sharpe ratio: -0.003842609463522613\n",
      "step 4, loss: 32.355621337890625, sharpe ratio: -0.006674338858493882\n",
      "step 5, loss: 32.313072204589844, sharpe ratio: -0.06325161482706891\n",
      "step 6, loss: 32.31388854980469, sharpe ratio: -0.030367276763949298\n",
      "step 7, loss: 32.3094482421875, sharpe ratio: -0.02063618267512358\n",
      "step 8, loss: 32.33231735229492, sharpe ratio: 0.08639039811220134\n",
      "step 9, loss: 32.32628631591797, sharpe ratio: 0.06466379488196096\n",
      "step 10, loss: 32.2999382019043, sharpe ratio: 0.03917309488377159\n",
      "step 11, loss: 32.28177261352539, sharpe ratio: 0.0089354148757852\n",
      "step 12, loss: 32.25400161743164, sharpe ratio: -0.018703231152505738\n",
      "validation sharpe ratio: 0.05934545098519352\n",
      "epoch 36: loss: 32.33952331542969, learning rate: 9.999998292187229e-05\n",
      "epoch 37...\n",
      "step 0, loss: 32.252891540527344, sharpe ratio: -0.013325395332476508\n",
      "step 1, loss: 32.26177978515625, sharpe ratio: 0.03912600254231841\n",
      "step 2, loss: 32.23707962036133, sharpe ratio: -0.0024939844559997773\n",
      "step 3, loss: 32.17010498046875, sharpe ratio: -0.09885582682661825\n",
      "step 4, loss: 32.219154357910156, sharpe ratio: -0.0014395696632449953\n",
      "step 5, loss: 32.2159423828125, sharpe ratio: 0.03843836381618502\n",
      "step 6, loss: 32.12476348876953, sharpe ratio: -0.07370819707675472\n",
      "step 7, loss: 32.17973709106445, sharpe ratio: 0.02002214652940766\n",
      "step 8, loss: 32.11619186401367, sharpe ratio: -0.06514255945499468\n",
      "step 9, loss: 32.11711120605469, sharpe ratio: -0.03961744188197657\n",
      "step 10, loss: 32.153541564941406, sharpe ratio: 0.04493121365524329\n",
      "step 11, loss: 32.114898681640625, sharpe ratio: -0.0035322911468906974\n",
      "step 12, loss: 32.18771743774414, sharpe ratio: 0.13432213298716872\n",
      "validation sharpe ratio: 0.06019520909496498\n",
      "epoch 37: loss: 32.18083953857422, learning rate: 9.999998292187229e-05\n",
      "epoch 38...\n",
      "step 0, loss: 32.09157943725586, sharpe ratio: -0.003540370046239077\n",
      "step 1, loss: 32.08633804321289, sharpe ratio: -0.002023590945265389\n",
      "step 2, loss: 32.03276824951172, sharpe ratio: -0.08393549978544942\n",
      "step 3, loss: 32.05125427246094, sharpe ratio: -0.007500839902313785\n",
      "step 4, loss: 32.0018196105957, sharpe ratio: -0.07978526044225498\n",
      "step 5, loss: 32.011531829833984, sharpe ratio: -0.026254707529519916\n",
      "step 6, loss: 32.04814529418945, sharpe ratio: 0.03382767768525343\n",
      "step 7, loss: 32.000946044921875, sharpe ratio: -0.010199171866787442\n",
      "step 8, loss: 31.972286224365234, sharpe ratio: -0.0429286935381475\n",
      "step 9, loss: 31.97646141052246, sharpe ratio: -0.012298988166208112\n",
      "step 10, loss: 31.950456619262695, sharpe ratio: -0.02995263406850749\n",
      "step 11, loss: 31.969816207885742, sharpe ratio: 0.029172197200779314\n",
      "step 12, loss: 31.958255767822266, sharpe ratio: 0.014315259007117097\n",
      "validation sharpe ratio: 0.05161471486982475\n",
      "epoch 38: loss: 32.01166534423828, learning rate: 9.999998292187229e-05\n",
      "epoch 39...\n",
      "step 0, loss: 31.90582275390625, sharpe ratio: -0.04754106762380561\n",
      "step 1, loss: 31.940214157104492, sharpe ratio: 0.03257515811359485\n",
      "step 2, loss: 31.898578643798828, sharpe ratio: -0.03557590274611103\n",
      "step 3, loss: 31.850730895996094, sharpe ratio: -0.09708117118686804\n",
      "step 4, loss: 31.855945587158203, sharpe ratio: -0.06408716566177869\n",
      "step 5, loss: 31.928144454956055, sharpe ratio: 0.05344336752383211\n",
      "step 6, loss: 31.832094192504883, sharpe ratio: -0.05044122547059182\n",
      "step 7, loss: 31.83487319946289, sharpe ratio: -0.029240503458385577\n",
      "step 8, loss: 31.85841941833496, sharpe ratio: 0.018602657122673782\n",
      "step 9, loss: 31.82120132446289, sharpe ratio: -0.019672065768888424\n",
      "step 10, loss: 31.840261459350586, sharpe ratio: 0.03750599645259459\n",
      "step 11, loss: 31.799680709838867, sharpe ratio: -0.02057104528339858\n",
      "step 12, loss: 31.770729064941406, sharpe ratio: -0.04574856531854294\n",
      "validation sharpe ratio: 0.050766165647577334\n",
      "epoch 39: loss: 31.85666847229004, learning rate: 9.999998292187229e-05\n",
      "epoch 40...\n",
      "step 0, loss: 31.815753936767578, sharpe ratio: 0.06624397620100224\n",
      "step 1, loss: 31.79983901977539, sharpe ratio: 0.03892277749508021\n",
      "step 2, loss: 31.806493759155273, sharpe ratio: 0.07357913698197704\n",
      "step 3, loss: 31.749980926513672, sharpe ratio: 0.00285152466733184\n",
      "step 4, loss: 31.747791290283203, sharpe ratio: 0.016007575444574578\n",
      "step 5, loss: 31.686214447021484, sharpe ratio: -0.07477763268442945\n",
      "step 6, loss: 31.75306510925293, sharpe ratio: 0.06947759620448711\n",
      "step 7, loss: 31.739957809448242, sharpe ratio: 0.04299382401603239\n",
      "step 8, loss: 31.685476303100586, sharpe ratio: -0.02423112007883048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9, loss: 31.699779510498047, sharpe ratio: 0.03498107507473312\n",
      "step 10, loss: 31.664844512939453, sharpe ratio: -0.009162072128386222\n",
      "step 11, loss: 31.683547973632812, sharpe ratio: 0.05017419096381587\n",
      "step 12, loss: 31.642427444458008, sharpe ratio: 0.007911163409951053\n",
      "validation sharpe ratio: 0.04828520553790877\n",
      "epoch 40: loss: 31.72886085510254, learning rate: 9.999998292187229e-05\n",
      "epoch 41...\n",
      "step 0, loss: 31.617874145507812, sharpe ratio: -0.049697845099332894\n",
      "step 1, loss: 31.585224151611328, sharpe ratio: -0.04992091444206156\n",
      "step 2, loss: 31.607280731201172, sharpe ratio: -0.001051102058122956\n",
      "step 3, loss: 31.647258758544922, sharpe ratio: 0.08637461587177792\n",
      "step 4, loss: 31.625476837158203, sharpe ratio: 0.06692317693404069\n",
      "step 5, loss: 31.548786163330078, sharpe ratio: -0.034199788478159965\n",
      "step 6, loss: 31.569292068481445, sharpe ratio: 0.024648686589549242\n",
      "step 7, loss: 31.560068130493164, sharpe ratio: 0.01989303931601314\n",
      "step 8, loss: 31.523841857910156, sharpe ratio: -0.02653153578853727\n",
      "step 9, loss: 31.565277099609375, sharpe ratio: 0.07338454682786902\n",
      "step 10, loss: 31.547443389892578, sharpe ratio: 0.05648654099529815\n",
      "step 11, loss: 31.476015090942383, sharpe ratio: -0.04317258594743289\n",
      "step 12, loss: 31.4897518157959, sharpe ratio: -0.00015699678262709373\n",
      "validation sharpe ratio: 0.05101577990362367\n",
      "epoch 41: loss: 31.566429138183594, learning rate: 9.999998292187229e-05\n",
      "epoch 42...\n",
      "step 0, loss: 31.44723892211914, sharpe ratio: -0.04154960168986794\n",
      "step 1, loss: 31.46100425720215, sharpe ratio: -0.009645849687689436\n",
      "step 2, loss: 31.40110969543457, sharpe ratio: -0.0920417638272081\n",
      "step 3, loss: 31.503185272216797, sharpe ratio: 0.10661746317971965\n",
      "step 4, loss: 31.407508850097656, sharpe ratio: -0.03044664457794065\n",
      "step 5, loss: 31.44192886352539, sharpe ratio: 0.028477193834784705\n",
      "step 6, loss: 31.373092651367188, sharpe ratio: -0.050483191432618084\n",
      "step 7, loss: 31.426427841186523, sharpe ratio: 0.05332770523348655\n",
      "step 8, loss: 31.376014709472656, sharpe ratio: -0.01352350472792043\n",
      "step 9, loss: 31.36607551574707, sharpe ratio: 0.0004426608747718254\n",
      "step 10, loss: 31.362873077392578, sharpe ratio: 0.015089181205748457\n",
      "step 11, loss: 31.34222412109375, sharpe ratio: -0.027899351821475283\n",
      "step 12, loss: 31.362876892089844, sharpe ratio: 0.04264911253347679\n",
      "validation sharpe ratio: 0.05272254825311484\n",
      "epoch 42: loss: 31.405506134033203, learning rate: 9.999998292187229e-05\n",
      "epoch 43...\n",
      "step 0, loss: 31.36079216003418, sharpe ratio: 0.05963158209833106\n",
      "step 1, loss: 31.295244216918945, sharpe ratio: -0.00904548416909294\n",
      "step 2, loss: 31.300020217895508, sharpe ratio: 0.0010902244972008108\n",
      "step 3, loss: 31.287199020385742, sharpe ratio: 0.0030480200946998506\n",
      "step 4, loss: 31.25581169128418, sharpe ratio: -0.027403647097871138\n",
      "step 5, loss: 31.231643676757812, sharpe ratio: -0.0475161791893873\n",
      "step 6, loss: 31.25752830505371, sharpe ratio: 0.012288860154253891\n",
      "step 7, loss: 31.252620697021484, sharpe ratio: 0.020893203253641066\n",
      "step 8, loss: 31.237085342407227, sharpe ratio: 0.02220994217476599\n",
      "step 9, loss: 31.219783782958984, sharpe ratio: 0.009043506081585396\n",
      "step 10, loss: 31.2294921875, sharpe ratio: 0.035261202623122406\n",
      "step 11, loss: 31.194360733032227, sharpe ratio: -0.004217777464438875\n",
      "step 12, loss: 31.19539451599121, sharpe ratio: 0.014453300496690483\n",
      "validation sharpe ratio: 0.05327726425242791\n",
      "epoch 43: loss: 31.25515365600586, learning rate: 9.999998292187229e-05\n",
      "epoch 44...\n",
      "step 0, loss: 31.20476531982422, sharpe ratio: 0.05536337166082994\n",
      "step 1, loss: 31.13165855407715, sharpe ratio: -0.04804350569068017\n",
      "step 2, loss: 31.138242721557617, sharpe ratio: -0.016062705286184763\n",
      "step 3, loss: 31.140363693237305, sharpe ratio: 0.00978501419910328\n",
      "step 4, loss: 31.041629791259766, sharpe ratio: -0.1423773951115281\n",
      "step 5, loss: 31.138639450073242, sharpe ratio: 0.0490426968089902\n",
      "step 6, loss: 31.107341766357422, sharpe ratio: -0.0007822900094816496\n",
      "step 7, loss: 31.060928344726562, sharpe ratio: -0.036725343002321116\n",
      "step 8, loss: 31.039777755737305, sharpe ratio: -0.049476622057858785\n",
      "step 9, loss: 31.073781967163086, sharpe ratio: 0.015002273372800367\n",
      "step 10, loss: 31.077716827392578, sharpe ratio: 0.058957728590748874\n",
      "step 11, loss: 31.069557189941406, sharpe ratio: 0.07331881213760207\n",
      "step 12, loss: 31.026729583740234, sharpe ratio: 0.003533667170835116\n",
      "validation sharpe ratio: 0.048133653270044\n",
      "epoch 44: loss: 31.096242904663086, learning rate: 9.999998292187229e-05\n",
      "epoch 45...\n",
      "step 0, loss: 30.983325958251953, sharpe ratio: -0.05556679324685783\n",
      "step 1, loss: 31.023271560668945, sharpe ratio: 0.024838795221904394\n",
      "step 2, loss: 30.999013900756836, sharpe ratio: 0.011546396263554257\n",
      "step 3, loss: 31.028606414794922, sharpe ratio: 0.07768677264428951\n",
      "step 4, loss: 30.986541748046875, sharpe ratio: 0.023945864184859757\n",
      "step 5, loss: 30.98149299621582, sharpe ratio: 0.035926107317890726\n",
      "step 6, loss: 30.959609985351562, sharpe ratio: 0.008837167072270127\n",
      "step 7, loss: 30.94071388244629, sharpe ratio: -3.844732799002104e-05\n",
      "step 8, loss: 30.954925537109375, sharpe ratio: 0.06396990556163908\n",
      "step 9, loss: 30.952709197998047, sharpe ratio: 0.07604920554490238\n",
      "step 10, loss: 30.849903106689453, sharpe ratio: -0.13311875662711925\n",
      "step 11, loss: 30.866907119750977, sharpe ratio: -0.03096999052516807\n",
      "step 12, loss: 30.88300132751465, sharpe ratio: 0.009356289504851267\n",
      "validation sharpe ratio: 0.048337152992349176\n",
      "epoch 45: loss: 30.954618453979492, learning rate: 9.999998292187229e-05\n",
      "epoch 46...\n",
      "step 0, loss: 30.899866104125977, sharpe ratio: 0.07420412752311768\n",
      "step 1, loss: 30.789485931396484, sharpe ratio: -0.1369098752797967\n",
      "step 2, loss: 30.831605911254883, sharpe ratio: -0.010616508054101473\n",
      "step 3, loss: 30.801406860351562, sharpe ratio: -0.05675561549479155\n",
      "step 4, loss: 30.801237106323242, sharpe ratio: -0.03829900564901298\n",
      "step 5, loss: 30.773717880249023, sharpe ratio: -0.055216187325553334\n",
      "step 6, loss: 30.846717834472656, sharpe ratio: 0.09061620117440486\n",
      "step 7, loss: 30.75556755065918, sharpe ratio: -0.027001552423492104\n",
      "step 8, loss: 30.859830856323242, sharpe ratio: 0.1280484348936447\n",
      "step 9, loss: 30.765077590942383, sharpe ratio: 0.021092527036917216\n",
      "step 10, loss: 30.72797203063965, sharpe ratio: -0.04221626709445503\n",
      "step 11, loss: 30.708650588989258, sharpe ratio: -0.048107481763044024\n",
      "step 12, loss: 30.727996826171875, sharpe ratio: 0.026352132119100383\n",
      "validation sharpe ratio: 0.04774628605914267\n",
      "epoch 46: loss: 30.79146957397461, learning rate: 9.999998292187229e-05\n",
      "epoch 47...\n",
      "step 0, loss: 30.781686782836914, sharpe ratio: 0.10817400169543706\n",
      "step 1, loss: 30.668869018554688, sharpe ratio: -0.05354878700916184\n",
      "step 2, loss: 30.721349716186523, sharpe ratio: 0.07173206934391177\n",
      "step 3, loss: 30.725421905517578, sharpe ratio: 0.09070191228244345\n",
      "step 4, loss: 30.643638610839844, sharpe ratio: -0.046620528174746684\n",
      "step 5, loss: 30.727380752563477, sharpe ratio: 0.11729330212344036\n",
      "step 6, loss: 30.67157554626465, sharpe ratio: 0.0558263314911349\n",
      "step 7, loss: 30.660602569580078, sharpe ratio: 0.0695596563944293\n",
      "step 8, loss: 30.68630599975586, sharpe ratio: 0.12348157885162477\n",
      "step 9, loss: 30.594999313354492, sharpe ratio: -0.025090491233336586\n",
      "step 10, loss: 30.585195541381836, sharpe ratio: -0.03076674032425298\n",
      "step 11, loss: 30.658679962158203, sharpe ratio: 0.125897475371248\n",
      "step 12, loss: 30.573535919189453, sharpe ratio: 0.003475366059510157\n",
      "validation sharpe ratio: 0.04655276398466512\n",
      "epoch 47: loss: 30.669172286987305, learning rate: 9.999998292187229e-05\n",
      "epoch 48...\n",
      "step 0, loss: 30.581523895263672, sharpe ratio: 0.024367701347588464\n",
      "step 1, loss: 30.523786544799805, sharpe ratio: -0.044771075083971934\n",
      "step 2, loss: 30.520389556884766, sharpe ratio: -0.04371528994769605\n",
      "step 3, loss: 30.488710403442383, sharpe ratio: -0.08673275712243299\n",
      "step 4, loss: 30.47721290588379, sharpe ratio: -0.07790750745736871\n",
      "step 5, loss: 30.544431686401367, sharpe ratio: 0.062331477717793314\n",
      "step 6, loss: 30.480438232421875, sharpe ratio: -0.010518661106299163\n",
      "step 7, loss: 30.50827980041504, sharpe ratio: 0.06258869405196897\n",
      "step 8, loss: 30.4904727935791, sharpe ratio: 0.038561664151058765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9, loss: 30.459753036499023, sharpe ratio: 0.005645845249113678\n",
      "step 10, loss: 30.4482364654541, sharpe ratio: 0.014598369428403438\n",
      "step 11, loss: 30.47268295288086, sharpe ratio: 0.04124783298209647\n",
      "step 12, loss: 30.42904281616211, sharpe ratio: 0.011142634801724536\n",
      "validation sharpe ratio: 0.04710587341400512\n",
      "epoch 48: loss: 30.494230270385742, learning rate: 9.999998292187229e-05\n",
      "epoch 49...\n",
      "step 0, loss: 30.428970336914062, sharpe ratio: 0.0013088826915766752\n",
      "step 1, loss: 30.392236709594727, sharpe ratio: -0.020024745855917996\n",
      "step 2, loss: 30.40444564819336, sharpe ratio: 0.03194257473345209\n",
      "step 3, loss: 30.405961990356445, sharpe ratio: 0.044142153453339106\n",
      "step 4, loss: 30.386672973632812, sharpe ratio: 0.041987625884381784\n",
      "step 5, loss: 30.382795333862305, sharpe ratio: 0.04498551076604382\n",
      "step 6, loss: 30.38501739501953, sharpe ratio: 0.06855344456816839\n",
      "step 7, loss: 30.32708740234375, sharpe ratio: -0.01009028110022659\n",
      "step 8, loss: 30.343265533447266, sharpe ratio: 0.044782766316329194\n",
      "step 9, loss: 30.306631088256836, sharpe ratio: 0.00415550621274649\n",
      "step 10, loss: 30.33147621154785, sharpe ratio: 0.04903626391534638\n",
      "step 11, loss: 30.24129867553711, sharpe ratio: -0.07487564634906252\n",
      "step 12, loss: 30.27853775024414, sharpe ratio: 0.015324753936693213\n",
      "validation sharpe ratio: 0.05073724606493156\n",
      "epoch 49: loss: 30.35495376586914, learning rate: 9.999998292187229e-05\n",
      "epoch 50...\n",
      "step 0, loss: 30.255626678466797, sharpe ratio: -0.005618385179412731\n",
      "step 1, loss: 30.28219985961914, sharpe ratio: 0.03776496512521433\n",
      "step 2, loss: 30.26515007019043, sharpe ratio: 0.03712175496515589\n",
      "step 3, loss: 30.207950592041016, sharpe ratio: -0.04066262902258232\n",
      "step 4, loss: 30.19843101501465, sharpe ratio: -0.06675056061029441\n",
      "step 5, loss: 30.247777938842773, sharpe ratio: 0.08795888292342008\n",
      "step 6, loss: 30.192262649536133, sharpe ratio: -0.015488504472183015\n",
      "step 7, loss: 30.21569061279297, sharpe ratio: 0.0806429836698225\n",
      "step 8, loss: 30.162704467773438, sharpe ratio: -0.008490397226960886\n",
      "step 9, loss: 30.20603370666504, sharpe ratio: 0.07290980404670218\n",
      "step 10, loss: 30.172122955322266, sharpe ratio: 0.03389299226473341\n",
      "step 11, loss: 30.200164794921875, sharpe ratio: 0.09241304262698326\n",
      "step 12, loss: 30.13682746887207, sharpe ratio: 0.01264595444183812\n",
      "validation sharpe ratio: 0.046546248639429244\n",
      "epoch 50: loss: 30.210994720458984, learning rate: 9.999998292187229e-05\n",
      "epoch 51...\n",
      "step 0, loss: 30.13884162902832, sharpe ratio: 0.07251763228107752\n",
      "step 1, loss: 30.102569580078125, sharpe ratio: -0.0053822070418294045\n",
      "step 2, loss: 30.101608276367188, sharpe ratio: 0.006295359536483831\n",
      "step 3, loss: 30.092620849609375, sharpe ratio: 0.04488740238079414\n",
      "step 4, loss: 30.024154663085938, sharpe ratio: -0.09760162550986176\n",
      "step 5, loss: 30.05332374572754, sharpe ratio: -0.008224448352672335\n",
      "step 6, loss: 30.038637161254883, sharpe ratio: -0.06932402333809953\n",
      "step 7, loss: 30.04424476623535, sharpe ratio: 0.010414305870823343\n",
      "step 8, loss: 30.050737380981445, sharpe ratio: 0.06279269290222997\n",
      "step 9, loss: 30.06256866455078, sharpe ratio: 0.08262272286123831\n",
      "step 10, loss: 29.99739646911621, sharpe ratio: -0.004543248629541596\n",
      "step 11, loss: 29.982189178466797, sharpe ratio: -0.037331637346317406\n",
      "step 12, loss: 29.979589462280273, sharpe ratio: -0.01703083423924128\n",
      "validation sharpe ratio: 0.051457848232538114\n",
      "epoch 51: loss: 30.051422119140625, learning rate: 9.999998292187229e-05\n",
      "epoch 52...\n",
      "step 0, loss: 29.92628288269043, sharpe ratio: -0.07782480845423172\n",
      "step 1, loss: 29.96633529663086, sharpe ratio: 0.013862600005916898\n",
      "step 2, loss: 29.944629669189453, sharpe ratio: 0.005477564975882686\n",
      "step 3, loss: 29.922019958496094, sharpe ratio: -0.024466017866841945\n",
      "step 4, loss: 29.934934616088867, sharpe ratio: 0.02175636957381384\n",
      "step 5, loss: 29.881389617919922, sharpe ratio: -0.04533013565601163\n",
      "step 6, loss: 29.901575088500977, sharpe ratio: 0.011233618690123152\n",
      "step 7, loss: 29.893754959106445, sharpe ratio: 0.04087361302261494\n",
      "step 8, loss: 29.89136505126953, sharpe ratio: 0.011849694942518915\n",
      "step 9, loss: 29.863876342773438, sharpe ratio: -0.006111665515405391\n",
      "step 10, loss: 29.854537963867188, sharpe ratio: -0.015641849042159997\n",
      "step 11, loss: 29.81955337524414, sharpe ratio: -0.03316142736802472\n",
      "step 12, loss: 29.8284854888916, sharpe ratio: 0.003233594416500025\n",
      "validation sharpe ratio: 0.04999789234246356\n",
      "epoch 52: loss: 29.894519805908203, learning rate: 9.999998292187229e-05\n",
      "epoch 53...\n",
      "step 0, loss: 29.84166717529297, sharpe ratio: 0.02928124447570144\n",
      "step 1, loss: 29.84288215637207, sharpe ratio: 0.06604785226096967\n",
      "step 2, loss: 29.780698776245117, sharpe ratio: -0.027001589730816872\n",
      "step 3, loss: 29.82587432861328, sharpe ratio: 0.05865966511010434\n",
      "step 4, loss: 29.769445419311523, sharpe ratio: -0.0018338893738678317\n",
      "step 5, loss: 29.733367919921875, sharpe ratio: -0.05449844479612195\n",
      "step 6, loss: 29.73689842224121, sharpe ratio: -0.02909427164016981\n",
      "step 7, loss: 29.70357322692871, sharpe ratio: -0.040528834451520834\n",
      "step 8, loss: 29.731096267700195, sharpe ratio: 0.007090468370316714\n",
      "step 9, loss: 29.68690299987793, sharpe ratio: -0.05831403275922384\n",
      "step 10, loss: 29.697067260742188, sharpe ratio: -0.02556996630210597\n",
      "step 11, loss: 29.72100257873535, sharpe ratio: 0.052819343540190355\n",
      "step 12, loss: 29.679288864135742, sharpe ratio: -0.005312602443988143\n",
      "validation sharpe ratio: 0.049902686331136745\n",
      "epoch 53: loss: 29.74997901916504, learning rate: 9.999998292187229e-05\n",
      "epoch 54...\n",
      "step 0, loss: 29.687734603881836, sharpe ratio: 0.021276061041891385\n",
      "step 1, loss: 29.63885498046875, sharpe ratio: -0.04433675336752557\n",
      "step 2, loss: 29.659086227416992, sharpe ratio: 0.0056276496840935355\n",
      "step 3, loss: 29.61545181274414, sharpe ratio: -0.035995771748214604\n",
      "step 4, loss: 29.651016235351562, sharpe ratio: 0.04832631812498707\n",
      "step 5, loss: 29.591121673583984, sharpe ratio: -0.04925050196596263\n",
      "step 6, loss: 29.618391036987305, sharpe ratio: 0.02376018941661889\n",
      "step 7, loss: 29.608827590942383, sharpe ratio: 0.0004966214183684918\n",
      "step 8, loss: 29.598817825317383, sharpe ratio: 0.01865799343732862\n",
      "step 9, loss: 29.608917236328125, sharpe ratio: 0.059242265425807655\n",
      "step 10, loss: 29.5079288482666, sharpe ratio: -0.1019141063424168\n",
      "step 11, loss: 29.546268463134766, sharpe ratio: 0.03734628383439913\n",
      "step 12, loss: 29.502111434936523, sharpe ratio: -0.0732220423941815\n",
      "validation sharpe ratio: 0.05523776688272795\n",
      "epoch 54: loss: 29.602657318115234, learning rate: 9.999998292187229e-05\n",
      "epoch 55...\n",
      "step 0, loss: 29.552413940429688, sharpe ratio: 0.044548962466337085\n",
      "step 1, loss: 29.55716896057129, sharpe ratio: 0.0824602609532862\n",
      "step 2, loss: 29.468570709228516, sharpe ratio: -0.04755685397383154\n",
      "step 3, loss: 29.498197555541992, sharpe ratio: 0.03247029786904407\n",
      "step 4, loss: 29.53000259399414, sharpe ratio: 0.06432424095553117\n",
      "step 5, loss: 29.449817657470703, sharpe ratio: -0.04588878566904349\n",
      "step 6, loss: 29.465845108032227, sharpe ratio: 0.01926408800411707\n",
      "step 7, loss: 29.44634246826172, sharpe ratio: -0.015894475361207545\n",
      "step 8, loss: 29.49386215209961, sharpe ratio: 0.11063146118786214\n",
      "step 9, loss: 29.41499900817871, sharpe ratio: -0.02568901653153329\n",
      "step 10, loss: 29.39984703063965, sharpe ratio: -0.04954714974190842\n",
      "step 11, loss: 29.38671875, sharpe ratio: -0.04124107350572371\n",
      "step 12, loss: 29.425565719604492, sharpe ratio: 0.06124617562689513\n",
      "validation sharpe ratio: 0.053000649406367555\n",
      "epoch 55: loss: 29.468412399291992, learning rate: 9.999998292187229e-05\n",
      "epoch 56...\n",
      "step 0, loss: 29.385639190673828, sharpe ratio: 0.017360662996109433\n",
      "step 1, loss: 29.346818923950195, sharpe ratio: -0.07903902252821389\n",
      "step 2, loss: 29.387636184692383, sharpe ratio: 0.059526037992140415\n",
      "step 3, loss: 29.348438262939453, sharpe ratio: -0.02722600964321238\n",
      "step 4, loss: 29.33607292175293, sharpe ratio: 0.01609116468227604\n",
      "step 5, loss: 29.344818115234375, sharpe ratio: 0.043316765393548735\n",
      "step 6, loss: 29.33407211303711, sharpe ratio: 0.05231486738130303\n",
      "step 7, loss: 29.29408836364746, sharpe ratio: -0.02461675909424849\n",
      "step 8, loss: 29.29637336730957, sharpe ratio: 0.04212740356917816\n",
      "step 9, loss: 29.275035858154297, sharpe ratio: -0.020122555265083462\n",
      "step 10, loss: 29.259244918823242, sharpe ratio: -0.02242039487312924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11, loss: 29.227123260498047, sharpe ratio: -0.037676907800350254\n",
      "step 12, loss: 29.257293701171875, sharpe ratio: 0.030764795521376988\n",
      "validation sharpe ratio: 0.05523776688272795\n",
      "epoch 56: loss: 29.3148193359375, learning rate: 9.999998292187229e-05\n",
      "epoch 57...\n",
      "step 0, loss: 29.254343032836914, sharpe ratio: 0.03096126139084385\n",
      "step 1, loss: 29.22684097290039, sharpe ratio: 0.014704764863149557\n",
      "step 2, loss: 29.221851348876953, sharpe ratio: 0.03409662969780584\n",
      "step 3, loss: 29.245054244995117, sharpe ratio: 0.0696815999223241\n",
      "step 4, loss: 29.191783905029297, sharpe ratio: -0.013557468628778805\n",
      "step 5, loss: 29.15711784362793, sharpe ratio: -0.036821119620526206\n",
      "step 6, loss: 29.14727783203125, sharpe ratio: -0.04572065651813463\n",
      "step 7, loss: 29.171802520751953, sharpe ratio: 0.059207643712545596\n",
      "step 8, loss: 29.17953109741211, sharpe ratio: 0.060577316243035056\n",
      "step 9, loss: 29.170652389526367, sharpe ratio: 0.08512020756168762\n",
      "step 10, loss: 29.139209747314453, sharpe ratio: 0.0355622454097536\n",
      "step 11, loss: 29.095735549926758, sharpe ratio: -0.02325552733590728\n",
      "step 12, loss: 29.129640579223633, sharpe ratio: 0.03968803481119541\n",
      "validation sharpe ratio: 0.05589364126025099\n",
      "epoch 57: loss: 29.17929458618164, learning rate: 9.999998292187229e-05\n",
      "epoch 58...\n",
      "step 0, loss: 29.024188995361328, sharpe ratio: -0.10375267408278316\n",
      "step 1, loss: 29.065837860107422, sharpe ratio: -0.048186606493479577\n",
      "step 2, loss: 29.06588363647461, sharpe ratio: 0.02167421806654134\n",
      "step 3, loss: 29.065505981445312, sharpe ratio: 0.011040624758641775\n",
      "step 4, loss: 29.037860870361328, sharpe ratio: -0.04017478477459838\n",
      "step 5, loss: 29.050071716308594, sharpe ratio: 0.018723750944533428\n",
      "step 6, loss: 29.0177001953125, sharpe ratio: -0.00398084294853675\n",
      "step 7, loss: 29.03851318359375, sharpe ratio: 0.0419477676696625\n",
      "step 8, loss: 28.984630584716797, sharpe ratio: -0.04042776779528299\n",
      "step 9, loss: 29.02626609802246, sharpe ratio: 0.08262981122347128\n",
      "step 10, loss: 29.00289535522461, sharpe ratio: 0.052271579831300256\n",
      "step 11, loss: 29.021177291870117, sharpe ratio: 0.10126276669467482\n",
      "step 12, loss: 29.003286361694336, sharpe ratio: 0.0809152276811675\n",
      "validation sharpe ratio: 0.0548618169085468\n",
      "epoch 58: loss: 29.031064987182617, learning rate: 9.999998292187229e-05\n",
      "epoch 59...\n",
      "step 0, loss: 28.93297576904297, sharpe ratio: 0.004477220068913406\n",
      "step 1, loss: 28.948545455932617, sharpe ratio: 0.021536693674961745\n",
      "step 2, loss: 28.89029312133789, sharpe ratio: -0.0607136998780456\n",
      "step 3, loss: 28.922258377075195, sharpe ratio: 0.014678942727057848\n",
      "step 4, loss: 28.943410873413086, sharpe ratio: 0.08486893038519237\n",
      "step 5, loss: 28.933002471923828, sharpe ratio: 0.0815195585456837\n",
      "step 6, loss: 28.875303268432617, sharpe ratio: -0.015041679603994043\n",
      "step 7, loss: 28.855770111083984, sharpe ratio: -0.022259646687395364\n",
      "step 8, loss: 28.887096405029297, sharpe ratio: 0.0671426780658476\n",
      "step 9, loss: 28.849233627319336, sharpe ratio: 0.003395087324259894\n",
      "step 10, loss: 28.84418487548828, sharpe ratio: 0.012637883268885593\n",
      "step 11, loss: 28.815242767333984, sharpe ratio: -0.020401759940198015\n",
      "step 12, loss: 28.82099723815918, sharpe ratio: 0.026433707721304725\n",
      "validation sharpe ratio: 0.0613015926164786\n",
      "epoch 59: loss: 28.886024475097656, learning rate: 9.999998292187229e-05\n",
      "epoch 60...\n",
      "step 0, loss: 28.82741928100586, sharpe ratio: 0.04422161620488302\n",
      "step 1, loss: 28.809663772583008, sharpe ratio: 0.04702738872216097\n",
      "step 2, loss: 28.790958404541016, sharpe ratio: 0.02555590564765389\n",
      "step 3, loss: 28.8080997467041, sharpe ratio: 0.09239160335439103\n",
      "step 4, loss: 28.767526626586914, sharpe ratio: 0.0105927149645577\n",
      "step 5, loss: 28.752639770507812, sharpe ratio: 0.0017449307793481542\n",
      "step 6, loss: 28.73773193359375, sharpe ratio: 0.0059716135334146395\n",
      "step 7, loss: 28.702177047729492, sharpe ratio: -0.036520619149736734\n",
      "step 8, loss: 28.779232025146484, sharpe ratio: 0.1302535666959126\n",
      "step 9, loss: 28.721750259399414, sharpe ratio: 0.0402056594485085\n",
      "step 10, loss: 28.720088958740234, sharpe ratio: 0.08173201847385327\n",
      "step 11, loss: 28.65520477294922, sharpe ratio: -0.056463978245975266\n",
      "step 12, loss: 28.666439056396484, sharpe ratio: -0.004560757994795557\n",
      "validation sharpe ratio: 0.058391864347139326\n",
      "epoch 60: loss: 28.749147415161133, learning rate: 9.999998292187229e-05\n",
      "epoch 61...\n",
      "step 0, loss: 28.6856689453125, sharpe ratio: 0.039980369361933575\n",
      "step 1, loss: 28.65859603881836, sharpe ratio: 0.03777286304920152\n",
      "step 2, loss: 28.616802215576172, sharpe ratio: -0.018908792248624094\n",
      "step 3, loss: 28.60188102722168, sharpe ratio: -0.06358896651344673\n",
      "step 4, loss: 28.601383209228516, sharpe ratio: 0.0006103063565879412\n",
      "step 5, loss: 28.563465118408203, sharpe ratio: -0.06183094591689357\n",
      "step 6, loss: 28.59369659423828, sharpe ratio: 0.013006110607733691\n",
      "step 7, loss: 28.58675765991211, sharpe ratio: 0.014995160419148126\n",
      "step 8, loss: 28.578258514404297, sharpe ratio: 0.021592174486236064\n",
      "step 9, loss: 28.562288284301758, sharpe ratio: 0.006003416750640558\n",
      "step 10, loss: 28.577482223510742, sharpe ratio: 0.0610437462833489\n",
      "step 11, loss: 28.5125732421875, sharpe ratio: -0.05521619320936476\n",
      "step 12, loss: 28.56724739074707, sharpe ratio: 0.07986380058751741\n",
      "validation sharpe ratio: 0.056233163527758256\n",
      "epoch 61: loss: 28.592777252197266, learning rate: 9.999998292187229e-05\n",
      "epoch 62...\n",
      "step 0, loss: 28.467388153076172, sharpe ratio: -0.08114839758750553\n",
      "step 1, loss: 28.489225387573242, sharpe ratio: -0.0488195840101754\n",
      "step 2, loss: 28.519535064697266, sharpe ratio: 0.059205828273554285\n",
      "step 3, loss: 28.526369094848633, sharpe ratio: 0.0637402267124598\n",
      "step 4, loss: 28.488971710205078, sharpe ratio: 0.04450647318195489\n",
      "step 5, loss: 28.450008392333984, sharpe ratio: -0.0024292171151566455\n",
      "step 6, loss: 28.466699600219727, sharpe ratio: 0.025477028446868614\n",
      "step 7, loss: 28.40863037109375, sharpe ratio: -0.045275977454211025\n",
      "step 8, loss: 28.475496292114258, sharpe ratio: 0.08619403164407985\n",
      "step 9, loss: 28.41609001159668, sharpe ratio: 0.018135812965026785\n",
      "step 10, loss: 28.394126892089844, sharpe ratio: -0.021255948951737387\n",
      "step 11, loss: 28.382366180419922, sharpe ratio: -0.026602168886253733\n",
      "step 12, loss: 28.39293670654297, sharpe ratio: 0.00817795423622501\n",
      "validation sharpe ratio: 0.056233163527758256\n",
      "epoch 62: loss: 28.45214080810547, learning rate: 9.999998292187229e-05\n",
      "epoch 63...\n",
      "step 0, loss: 28.40009307861328, sharpe ratio: 0.049074146904253636\n",
      "step 1, loss: 28.380849838256836, sharpe ratio: 0.035747918714091595\n",
      "step 2, loss: 28.37190818786621, sharpe ratio: 0.0512500903601632\n",
      "step 3, loss: 28.394195556640625, sharpe ratio: 0.11111729968235942\n",
      "step 4, loss: 28.307100296020508, sharpe ratio: -0.04510551285764272\n",
      "step 5, loss: 28.320098876953125, sharpe ratio: 0.002984904820307605\n",
      "step 6, loss: 28.312070846557617, sharpe ratio: 0.010816766707845732\n",
      "step 7, loss: 28.2978515625, sharpe ratio: 0.006496405385429152\n",
      "step 8, loss: 28.300514221191406, sharpe ratio: -0.005577237636940926\n",
      "step 9, loss: 28.286827087402344, sharpe ratio: 0.018016398819652214\n",
      "step 10, loss: 28.26850128173828, sharpe ratio: 0.006579362768959612\n",
      "step 11, loss: 28.279977798461914, sharpe ratio: 0.0771144666612976\n",
      "step 12, loss: 28.199426651000977, sharpe ratio: -0.08649212533343464\n",
      "validation sharpe ratio: 0.05787698091988012\n",
      "epoch 63: loss: 28.316877365112305, learning rate: 9.999998292187229e-05\n",
      "epoch 64...\n",
      "step 0, loss: 28.19837760925293, sharpe ratio: -0.05077619185100863\n",
      "step 1, loss: 28.2343692779541, sharpe ratio: 0.001187355849483724\n",
      "step 2, loss: 28.23921012878418, sharpe ratio: 0.03815883088815862\n",
      "step 3, loss: 28.22942543029785, sharpe ratio: 0.08446403228317934\n",
      "step 4, loss: 28.17292022705078, sharpe ratio: -0.031459707177496604\n",
      "step 5, loss: 28.178916931152344, sharpe ratio: 0.001100835096224894\n",
      "step 6, loss: 28.176252365112305, sharpe ratio: 0.009553148751806403\n",
      "step 7, loss: 28.139158248901367, sharpe ratio: -0.033911953687670374\n",
      "step 8, loss: 28.116727828979492, sharpe ratio: -0.0394777981108427\n",
      "step 9, loss: 28.155820846557617, sharpe ratio: 0.07487010934607244\n",
      "step 10, loss: 28.15913963317871, sharpe ratio: 0.08192649962964098\n",
      "step 11, loss: 28.091907501220703, sharpe ratio: -0.037860461484240035\n",
      "step 12, loss: 28.092756271362305, sharpe ratio: -0.01840483509590069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation sharpe ratio: 0.057360037749062016\n",
      "epoch 64: loss: 28.16807746887207, learning rate: 9.999998292187229e-05\n",
      "epoch 65...\n",
      "step 0, loss: 28.06717872619629, sharpe ratio: -0.06350049761083168\n",
      "step 1, loss: 28.06934928894043, sharpe ratio: -0.020878833428709193\n",
      "step 2, loss: 28.062192916870117, sharpe ratio: -0.022998970617654976\n",
      "step 3, loss: 28.07716941833496, sharpe ratio: 0.03662777513748808\n",
      "step 4, loss: 28.059123992919922, sharpe ratio: 0.02029719440301316\n",
      "step 5, loss: 28.045385360717773, sharpe ratio: 0.04869956553526184\n",
      "step 6, loss: 28.010255813598633, sharpe ratio: -0.027572870401041397\n",
      "step 7, loss: 28.031587600708008, sharpe ratio: 0.05202351460170505\n",
      "step 8, loss: 28.03778648376465, sharpe ratio: 0.08144807996558134\n",
      "step 9, loss: 27.996843338012695, sharpe ratio: 0.010083912412099398\n",
      "step 10, loss: 27.987075805664062, sharpe ratio: -0.001428122970084711\n",
      "step 11, loss: 27.95339012145996, sharpe ratio: -0.0384908718111191\n",
      "step 12, loss: 27.977781295776367, sharpe ratio: 0.06436299426365225\n",
      "validation sharpe ratio: 0.0584301947697884\n",
      "epoch 65: loss: 28.028858184814453, learning rate: 9.999998292187229e-05\n",
      "epoch 66...\n",
      "step 0, loss: 27.980985641479492, sharpe ratio: 0.07056676598133077\n",
      "step 1, loss: 27.965713500976562, sharpe ratio: 0.041130800165291116\n",
      "step 2, loss: 27.94271469116211, sharpe ratio: 0.015899724156597673\n",
      "step 3, loss: 27.92829132080078, sharpe ratio: 0.018471311581617703\n",
      "step 4, loss: 27.896833419799805, sharpe ratio: -0.017522914534731305\n",
      "step 5, loss: 27.88853645324707, sharpe ratio: -0.011356703914523782\n",
      "step 6, loss: 27.88480567932129, sharpe ratio: 0.004231756263377214\n",
      "step 7, loss: 27.876924514770508, sharpe ratio: 0.008013821931331756\n",
      "step 8, loss: 27.85005760192871, sharpe ratio: -0.04211395871376231\n",
      "step 9, loss: 27.86376190185547, sharpe ratio: 0.006837213021651653\n",
      "step 10, loss: 27.858911514282227, sharpe ratio: 0.03592418235103927\n",
      "step 11, loss: 27.832664489746094, sharpe ratio: 0.02491422086056317\n",
      "step 12, loss: 27.779945373535156, sharpe ratio: -0.07834539977028582\n",
      "validation sharpe ratio: 0.059595196194018145\n",
      "epoch 66: loss: 27.888471603393555, learning rate: 9.999998292187229e-05\n",
      "epoch 67...\n",
      "step 0, loss: 27.840789794921875, sharpe ratio: 0.05187700917856369\n",
      "step 1, loss: 27.767074584960938, sharpe ratio: -0.058287787907472306\n",
      "step 2, loss: 27.789487838745117, sharpe ratio: 0.01967120799060569\n",
      "step 3, loss: 27.77754783630371, sharpe ratio: 0.0045841337609084125\n",
      "step 4, loss: 27.750795364379883, sharpe ratio: -0.01799231767545321\n",
      "step 5, loss: 27.75126838684082, sharpe ratio: 0.005542451065942908\n",
      "step 6, loss: 27.739707946777344, sharpe ratio: -0.006225023913600313\n",
      "step 7, loss: 27.761638641357422, sharpe ratio: 0.07438445516580795\n",
      "step 8, loss: 27.71931266784668, sharpe ratio: 0.016211029890276882\n",
      "step 9, loss: 27.700780868530273, sharpe ratio: -0.009831036814822947\n",
      "step 10, loss: 27.714628219604492, sharpe ratio: 0.043150148011900105\n",
      "step 11, loss: 27.70555877685547, sharpe ratio: 0.049942280676296635\n",
      "step 12, loss: 27.716096878051758, sharpe ratio: 0.09503368935467349\n",
      "validation sharpe ratio: 0.05631281289510179\n",
      "epoch 67: loss: 27.748823165893555, learning rate: 9.999998292187229e-05\n",
      "epoch 68...\n",
      "step 0, loss: 27.680068969726562, sharpe ratio: 0.021027463743483195\n",
      "step 1, loss: 27.656871795654297, sharpe ratio: 0.00934413072846604\n",
      "step 2, loss: 27.63664436340332, sharpe ratio: -0.006434649984765652\n",
      "step 3, loss: 27.63056755065918, sharpe ratio: -0.004725351400118888\n",
      "step 4, loss: 27.59397315979004, sharpe ratio: -0.07073828284250484\n",
      "step 5, loss: 27.653669357299805, sharpe ratio: 0.06691441196764164\n",
      "step 6, loss: 27.614761352539062, sharpe ratio: 0.036584730433423876\n",
      "step 7, loss: 27.57796859741211, sharpe ratio: -0.02530298333682578\n",
      "step 8, loss: 27.61845588684082, sharpe ratio: 0.07702184160129123\n",
      "step 9, loss: 27.603862762451172, sharpe ratio: 0.042036490719086134\n",
      "step 10, loss: 27.568206787109375, sharpe ratio: 0.012655298764664426\n",
      "step 11, loss: 27.58998680114746, sharpe ratio: 0.06285125274531968\n",
      "step 12, loss: 27.550920486450195, sharpe ratio: 0.04363931448002115\n",
      "validation sharpe ratio: 0.055869761505598074\n",
      "epoch 68: loss: 27.613536834716797, learning rate: 9.999998292187229e-05\n",
      "epoch 69...\n",
      "step 0, loss: 27.525177001953125, sharpe ratio: -0.008642325513741342\n",
      "step 1, loss: 27.519899368286133, sharpe ratio: 0.022734849005838484\n",
      "step 2, loss: 27.521860122680664, sharpe ratio: 0.008744264338012531\n",
      "step 3, loss: 27.467899322509766, sharpe ratio: -0.051370228898115655\n",
      "step 4, loss: 27.451608657836914, sharpe ratio: -0.0591679228839217\n",
      "step 5, loss: 27.441844940185547, sharpe ratio: -0.0504358259572935\n",
      "step 6, loss: 27.45558738708496, sharpe ratio: -0.013566170718367944\n",
      "step 7, loss: 27.485321044921875, sharpe ratio: 0.05786544675781442\n",
      "step 8, loss: 27.46617889404297, sharpe ratio: 0.043563737674038414\n",
      "step 9, loss: 27.434778213500977, sharpe ratio: 0.00926762607014018\n",
      "step 10, loss: 27.408649444580078, sharpe ratio: -0.015936199325873538\n",
      "step 11, loss: 27.41981315612793, sharpe ratio: 0.02562422574563137\n",
      "step 12, loss: 27.440082550048828, sharpe ratio: 0.09785593101600175\n",
      "validation sharpe ratio: 0.053825398921501004\n",
      "epoch 69: loss: 27.46451759338379, learning rate: 9.999998292187229e-05\n",
      "epoch 70...\n",
      "step 0, loss: 27.42388153076172, sharpe ratio: 0.06048546685077376\n",
      "step 1, loss: 27.35793685913086, sharpe ratio: -0.06693328800886447\n",
      "step 2, loss: 27.382741928100586, sharpe ratio: 0.02666131436709656\n",
      "step 3, loss: 27.370866775512695, sharpe ratio: 0.025654508696488464\n",
      "step 4, loss: 27.373294830322266, sharpe ratio: 0.04776774621519065\n",
      "step 5, loss: 27.336416244506836, sharpe ratio: 0.0005831288665970671\n",
      "step 6, loss: 27.340768814086914, sharpe ratio: 0.029966026393311695\n",
      "step 7, loss: 27.369659423828125, sharpe ratio: 0.1079293441868592\n",
      "step 8, loss: 27.296327590942383, sharpe ratio: -0.006841396734897056\n",
      "step 9, loss: 27.279245376586914, sharpe ratio: -0.03403194739032846\n",
      "step 10, loss: 27.326295852661133, sharpe ratio: 0.07104183076575515\n",
      "step 11, loss: 27.25181007385254, sharpe ratio: -0.07263634116517743\n",
      "step 12, loss: 27.231657028198242, sharpe ratio: -0.0557172778567947\n",
      "validation sharpe ratio: 0.04870176470014816\n",
      "epoch 70: loss: 27.333913803100586, learning rate: 9.999998292187229e-05\n",
      "epoch 71...\n",
      "step 0, loss: 27.26468849182129, sharpe ratio: 0.014164962720366473\n",
      "step 1, loss: 27.257551193237305, sharpe ratio: 0.06368709354130508\n",
      "step 2, loss: 27.220706939697266, sharpe ratio: -0.022119926869613567\n",
      "step 3, loss: 27.197509765625, sharpe ratio: -0.030988516690536115\n",
      "step 4, loss: 27.210145950317383, sharpe ratio: 0.0011055767153029489\n",
      "step 5, loss: 27.18900489807129, sharpe ratio: -0.022458808726072026\n",
      "step 6, loss: 27.207815170288086, sharpe ratio: 0.04678899705048436\n",
      "step 7, loss: 27.196765899658203, sharpe ratio: 0.03661028869189044\n",
      "step 8, loss: 27.186477661132812, sharpe ratio: 0.06706118826327863\n",
      "step 9, loss: 27.172149658203125, sharpe ratio: 0.024802801458202864\n",
      "step 10, loss: 27.153064727783203, sharpe ratio: 0.031457602054278874\n",
      "step 11, loss: 27.157791137695312, sharpe ratio: 0.057564766735770864\n",
      "step 12, loss: 27.134273529052734, sharpe ratio: 0.009547579196381852\n",
      "validation sharpe ratio: 0.04926068366123582\n",
      "epoch 71: loss: 27.19599723815918, learning rate: 9.999998292187229e-05\n",
      "epoch 72...\n",
      "step 0, loss: 27.115650177001953, sharpe ratio: 0.013461726818291667\n",
      "step 1, loss: 27.09198570251465, sharpe ratio: -0.02148636923644153\n",
      "step 2, loss: 27.08099937438965, sharpe ratio: 0.006709643236446135\n",
      "step 3, loss: 27.09052848815918, sharpe ratio: 0.01600112814275107\n",
      "step 4, loss: 27.054384231567383, sharpe ratio: -0.03354974084174655\n",
      "step 5, loss: 27.047740936279297, sharpe ratio: -0.03240378584169164\n",
      "step 6, loss: 27.093828201293945, sharpe ratio: 0.09413479251839177\n",
      "step 7, loss: 27.040668487548828, sharpe ratio: -0.0027889326782497264\n",
      "step 8, loss: 27.069005966186523, sharpe ratio: 0.07116439757068388\n",
      "step 9, loss: 27.026643753051758, sharpe ratio: 0.010168333981447923\n",
      "step 10, loss: 27.021068572998047, sharpe ratio: 0.025798293350151144\n",
      "step 11, loss: 26.97445297241211, sharpe ratio: -0.0360512073465018\n",
      "step 12, loss: 26.958646774291992, sharpe ratio: -0.05491673108522808\n",
      "validation sharpe ratio: 0.049953412807126285\n",
      "epoch 72: loss: 27.051198959350586, learning rate: 9.999998292187229e-05\n",
      "epoch 73...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 26.966894149780273, sharpe ratio: -0.016339272305736507\n",
      "step 1, loss: 26.953577041625977, sharpe ratio: -0.019886390985158778\n",
      "step 2, loss: 26.935848236083984, sharpe ratio: -0.031492407376024205\n",
      "step 3, loss: 26.95914077758789, sharpe ratio: 0.017576804867285342\n",
      "step 4, loss: 26.938007354736328, sharpe ratio: 0.006907561424474674\n",
      "step 5, loss: 26.967788696289062, sharpe ratio: 0.07543046682474433\n",
      "step 6, loss: 26.932125091552734, sharpe ratio: 0.02291983031602889\n",
      "step 7, loss: 26.919591903686523, sharpe ratio: 0.038249611560281234\n",
      "step 8, loss: 26.904972076416016, sharpe ratio: 0.029036563939692395\n",
      "step 9, loss: 26.877470016479492, sharpe ratio: -0.023994309139594237\n",
      "step 10, loss: 26.845701217651367, sharpe ratio: -0.00883944693878149\n",
      "step 11, loss: 26.869949340820312, sharpe ratio: 0.04431414421424028\n",
      "step 12, loss: 26.86118507385254, sharpe ratio: 0.0205833988906043\n",
      "validation sharpe ratio: 0.05126905305837832\n",
      "epoch 73: loss: 26.917863845825195, learning rate: 9.999998292187229e-05\n",
      "epoch 74...\n",
      "step 0, loss: 26.829627990722656, sharpe ratio: -0.01595707001160236\n",
      "step 1, loss: 26.834814071655273, sharpe ratio: 0.015171549682754477\n",
      "step 2, loss: 26.81771469116211, sharpe ratio: 0.001960649706608683\n",
      "step 3, loss: 26.808086395263672, sharpe ratio: 0.026711643715694428\n",
      "step 4, loss: 26.845041275024414, sharpe ratio: 0.09061798504702684\n",
      "step 5, loss: 26.85201072692871, sharpe ratio: 0.13133707057814065\n",
      "step 6, loss: 26.76903533935547, sharpe ratio: -0.0037933723307265743\n",
      "step 7, loss: 26.78106117248535, sharpe ratio: 0.027207537809267768\n",
      "step 8, loss: 26.747638702392578, sharpe ratio: -0.0163136765870234\n",
      "step 9, loss: 26.755512237548828, sharpe ratio: 0.008957313521854329\n",
      "step 10, loss: 26.73710060119629, sharpe ratio: 0.008073791019066728\n",
      "step 11, loss: 26.725879669189453, sharpe ratio: 0.021284962088127104\n",
      "step 12, loss: 26.704145431518555, sharpe ratio: -0.03606407260205301\n",
      "validation sharpe ratio: 0.054855706982741975\n",
      "epoch 74: loss: 26.785205841064453, learning rate: 9.999998292187229e-05\n",
      "epoch 75...\n",
      "step 0, loss: 26.70618438720703, sharpe ratio: 0.008336514703930455\n",
      "step 1, loss: 26.70574378967285, sharpe ratio: 0.027046447131845855\n",
      "step 2, loss: 26.687047958374023, sharpe ratio: 0.028806760578908963\n",
      "step 3, loss: 26.742765426635742, sharpe ratio: 0.12173085745803783\n",
      "step 4, loss: 26.698816299438477, sharpe ratio: 0.04951868822109666\n",
      "step 5, loss: 26.710590362548828, sharpe ratio: 0.10052841967323688\n",
      "step 6, loss: 26.621267318725586, sharpe ratio: -0.036197189573057384\n",
      "step 7, loss: 26.649555206298828, sharpe ratio: 0.031022385377328508\n",
      "step 8, loss: 26.615859985351562, sharpe ratio: -0.010621616170847985\n",
      "step 9, loss: 26.604736328125, sharpe ratio: -0.002561247672772708\n",
      "step 10, loss: 26.619644165039062, sharpe ratio: 0.04645065214358437\n",
      "step 11, loss: 26.603221893310547, sharpe ratio: 0.03358233618481613\n",
      "step 12, loss: 26.586172103881836, sharpe ratio: 0.017543463941952688\n",
      "validation sharpe ratio: 0.05317338479794374\n",
      "epoch 75: loss: 26.657814025878906, learning rate: 9.999998292187229e-05\n",
      "epoch 76...\n",
      "step 0, loss: 26.549381256103516, sharpe ratio: -0.02672572701816646\n",
      "step 1, loss: 26.53185272216797, sharpe ratio: -0.05451071748336593\n",
      "step 2, loss: 26.57083511352539, sharpe ratio: 0.09260716290245659\n",
      "step 3, loss: 26.56055450439453, sharpe ratio: 0.06258539986415745\n",
      "step 4, loss: 26.528474807739258, sharpe ratio: 0.029860953731373256\n",
      "step 5, loss: 26.48798942565918, sharpe ratio: -0.03818818380950937\n",
      "step 6, loss: 26.49591636657715, sharpe ratio: 0.006003247336922483\n",
      "step 7, loss: 26.512691497802734, sharpe ratio: 0.04530221655953505\n",
      "step 8, loss: 26.47494888305664, sharpe ratio: -0.001882850933563619\n",
      "step 9, loss: 26.470317840576172, sharpe ratio: 0.02458592127692877\n",
      "step 10, loss: 26.471281051635742, sharpe ratio: 0.009606872412352627\n",
      "step 11, loss: 26.447803497314453, sharpe ratio: -0.01788896418195409\n",
      "step 12, loss: 26.44011116027832, sharpe ratio: -0.01794546338896409\n",
      "validation sharpe ratio: 0.06115827118237404\n",
      "epoch 76: loss: 26.503244400024414, learning rate: 9.999998292187229e-05\n",
      "epoch 77...\n",
      "step 0, loss: 26.492252349853516, sharpe ratio: 0.1003302451517405\n",
      "step 1, loss: 26.428009033203125, sharpe ratio: 0.03109917521747259\n",
      "step 2, loss: 26.42974090576172, sharpe ratio: 0.029668383123407555\n",
      "step 3, loss: 26.405641555786133, sharpe ratio: 0.0072283428292302075\n",
      "step 4, loss: 26.37065315246582, sharpe ratio: -0.04564652515164106\n",
      "step 5, loss: 26.382722854614258, sharpe ratio: 0.007825317655471786\n",
      "step 6, loss: 26.408100128173828, sharpe ratio: 0.08603928802057634\n",
      "step 7, loss: 26.38225746154785, sharpe ratio: 0.05832548045917332\n",
      "step 8, loss: 26.328510284423828, sharpe ratio: -0.026784737702561626\n",
      "step 9, loss: 26.36640739440918, sharpe ratio: 0.05410423033060162\n",
      "step 10, loss: 26.319799423217773, sharpe ratio: -0.01164145557328398\n",
      "step 11, loss: 26.369897842407227, sharpe ratio: 0.09562292349660066\n",
      "step 12, loss: 26.313507080078125, sharpe ratio: 0.013161620423766084\n",
      "validation sharpe ratio: 0.05940400702934605\n",
      "epoch 77: loss: 26.384422302246094, learning rate: 9.999998292187229e-05\n",
      "epoch 78...\n",
      "step 0, loss: 26.321189880371094, sharpe ratio: 0.04983578951673159\n",
      "step 1, loss: 26.270999908447266, sharpe ratio: -0.009506807402111631\n",
      "step 2, loss: 26.2932071685791, sharpe ratio: 0.038619797705348874\n",
      "step 3, loss: 26.297739028930664, sharpe ratio: 0.08134308911309107\n",
      "step 4, loss: 26.266820907592773, sharpe ratio: 0.026836158158222222\n",
      "step 5, loss: 26.26529312133789, sharpe ratio: 0.028882113607622092\n",
      "step 6, loss: 26.23030662536621, sharpe ratio: 0.007796865211602591\n",
      "step 7, loss: 26.225929260253906, sharpe ratio: 0.0034330370351669205\n",
      "step 8, loss: 26.265304565429688, sharpe ratio: 0.11934934889259897\n",
      "step 9, loss: 26.16693878173828, sharpe ratio: -0.06540733946868818\n",
      "step 10, loss: 26.213443756103516, sharpe ratio: 0.04806911874094724\n",
      "step 11, loss: 26.15704917907715, sharpe ratio: -0.05013182197716655\n",
      "step 12, loss: 26.16251564025879, sharpe ratio: -0.017319944765832207\n",
      "validation sharpe ratio: 0.056402582647450766\n",
      "epoch 78: loss: 26.24128532409668, learning rate: 9.999998292187229e-05\n",
      "epoch 79...\n",
      "step 0, loss: 26.1345272064209, sharpe ratio: -0.0420348936517434\n",
      "step 1, loss: 26.167198181152344, sharpe ratio: 0.022809694870461274\n",
      "step 2, loss: 26.13772964477539, sharpe ratio: -0.0007123905464659283\n",
      "step 3, loss: 26.11235809326172, sharpe ratio: -0.03877463847178304\n",
      "step 4, loss: 26.090938568115234, sharpe ratio: -0.045532718991745116\n",
      "step 5, loss: 26.0870361328125, sharpe ratio: -0.0628710684630309\n",
      "step 6, loss: 26.12801742553711, sharpe ratio: 0.05891336436728779\n",
      "step 7, loss: 26.114315032958984, sharpe ratio: 0.0249314455176602\n",
      "step 8, loss: 26.112546920776367, sharpe ratio: 0.08572017020808419\n",
      "step 9, loss: 26.075075149536133, sharpe ratio: 0.01154255011841389\n",
      "step 10, loss: 26.066991806030273, sharpe ratio: 0.024747069670200583\n",
      "step 11, loss: 26.03363800048828, sharpe ratio: -0.024203208869261382\n",
      "step 12, loss: 26.04233169555664, sharpe ratio: 0.015592762697777865\n",
      "validation sharpe ratio: 0.05539272181711413\n",
      "epoch 79: loss: 26.100208282470703, learning rate: 9.999998292187229e-05\n",
      "epoch 80...\n",
      "step 0, loss: 26.0378475189209, sharpe ratio: 0.037761598345109636\n",
      "step 1, loss: 26.016111373901367, sharpe ratio: -0.02738499240638616\n",
      "step 2, loss: 26.00856590270996, sharpe ratio: 0.019651014247330987\n",
      "step 3, loss: 25.99618148803711, sharpe ratio: -0.0028354046264131086\n",
      "step 4, loss: 25.99957847595215, sharpe ratio: 0.043993980761610256\n",
      "step 5, loss: 25.997604370117188, sharpe ratio: 0.03666259243687732\n",
      "step 6, loss: 25.958776473999023, sharpe ratio: 0.0073021069457984665\n",
      "step 7, loss: 25.99040412902832, sharpe ratio: 0.07488484016804807\n",
      "step 8, loss: 25.985702514648438, sharpe ratio: 0.0695472895883933\n",
      "step 9, loss: 25.904226303100586, sharpe ratio: -0.04873020999571687\n",
      "step 10, loss: 25.927030563354492, sharpe ratio: 0.009471130619086499\n",
      "step 11, loss: 25.908533096313477, sharpe ratio: -0.014672555693638274\n",
      "step 12, loss: 25.882444381713867, sharpe ratio: -0.03199045750318605\n",
      "validation sharpe ratio: 0.054406696583635616\n",
      "epoch 80: loss: 25.970232009887695, learning rate: 9.999998292187229e-05\n",
      "epoch 81...\n",
      "step 0, loss: 25.926523208618164, sharpe ratio: 0.06067073818418008\n",
      "step 1, loss: 25.85700225830078, sharpe ratio: -0.05654420145254681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2, loss: 25.836790084838867, sharpe ratio: -0.1332146865869998\n",
      "step 3, loss: 25.873167037963867, sharpe ratio: 0.02029297687333008\n",
      "step 4, loss: 25.888200759887695, sharpe ratio: 0.060402001506341244\n",
      "step 5, loss: 25.867321014404297, sharpe ratio: 0.056596531649978785\n",
      "step 6, loss: 25.85173225402832, sharpe ratio: 0.04498026558323587\n",
      "step 7, loss: 25.78901481628418, sharpe ratio: -0.06415542851500457\n",
      "step 8, loss: 25.80239486694336, sharpe ratio: -0.0051439490874916095\n",
      "step 9, loss: 25.816965103149414, sharpe ratio: 0.02879706829086894\n",
      "step 10, loss: 25.78486442565918, sharpe ratio: -0.016323737975187116\n",
      "step 11, loss: 25.804649353027344, sharpe ratio: 0.05322042882817289\n",
      "step 12, loss: 25.76632308959961, sharpe ratio: 0.008694177971325528\n",
      "validation sharpe ratio: 0.051177302738261335\n",
      "epoch 81: loss: 25.835765838623047, learning rate: 9.999998292187229e-05\n",
      "epoch 82...\n",
      "step 0, loss: 25.706748962402344, sharpe ratio: -0.09127479516262713\n",
      "step 1, loss: 25.724340438842773, sharpe ratio: -0.014093360701120904\n",
      "step 2, loss: 25.682817459106445, sharpe ratio: -0.09008302667950681\n",
      "step 3, loss: 25.70274543762207, sharpe ratio: -0.056748511954699515\n",
      "step 4, loss: 25.725706100463867, sharpe ratio: 0.007691598347553895\n",
      "step 5, loss: 25.752918243408203, sharpe ratio: 0.0966359450770363\n",
      "step 6, loss: 25.699581146240234, sharpe ratio: 0.013012824303440817\n",
      "step 7, loss: 25.698486328125, sharpe ratio: 0.002103454042764184\n",
      "step 8, loss: 25.66377830505371, sharpe ratio: 0.0022666378356736086\n",
      "step 9, loss: 25.673566818237305, sharpe ratio: 0.020707154782760934\n",
      "step 10, loss: 25.6665096282959, sharpe ratio: 0.029789757753062085\n",
      "step 11, loss: 25.638166427612305, sharpe ratio: -0.0027956086780178517\n",
      "step 12, loss: 25.64678382873535, sharpe ratio: 0.012744390990890343\n",
      "validation sharpe ratio: 0.05726601205504888\n",
      "epoch 82: loss: 25.69093132019043, learning rate: 9.999998292187229e-05\n",
      "epoch 83...\n",
      "step 0, loss: 25.620067596435547, sharpe ratio: 0.029650394157398126\n",
      "step 1, loss: 25.613842010498047, sharpe ratio: -0.027543859574788717\n",
      "step 2, loss: 25.64127540588379, sharpe ratio: 0.08488092380954636\n",
      "step 3, loss: 25.601268768310547, sharpe ratio: 0.002985018528571772\n",
      "step 4, loss: 25.6185302734375, sharpe ratio: 0.03179211083499614\n",
      "step 5, loss: 25.57968521118164, sharpe ratio: 0.011027749828290481\n",
      "step 6, loss: 25.56163787841797, sharpe ratio: -0.0004066282915047496\n",
      "step 7, loss: 25.5604190826416, sharpe ratio: 0.008834603615661666\n",
      "step 8, loss: 25.560317993164062, sharpe ratio: 0.025987555474804882\n",
      "step 9, loss: 25.550024032592773, sharpe ratio: 0.02907066785354026\n",
      "step 10, loss: 25.541887283325195, sharpe ratio: 0.029420791543968407\n",
      "step 11, loss: 25.525205612182617, sharpe ratio: 0.020025677248843146\n",
      "step 12, loss: 25.49982261657715, sharpe ratio: -0.009741542002769577\n",
      "validation sharpe ratio: 0.058247354345142985\n",
      "epoch 83: loss: 25.574920654296875, learning rate: 9.999998292187229e-05\n",
      "epoch 84...\n",
      "step 0, loss: 25.480749130249023, sharpe ratio: -0.033081217606262335\n",
      "step 1, loss: 25.463899612426758, sharpe ratio: -0.04357216210763105\n",
      "step 2, loss: 25.51236343383789, sharpe ratio: 0.07086920094809761\n",
      "step 3, loss: 25.46200942993164, sharpe ratio: 0.0057896316188254734\n",
      "step 4, loss: 25.457717895507812, sharpe ratio: 0.015306644047421374\n",
      "step 5, loss: 25.432086944580078, sharpe ratio: -0.02225814581122511\n",
      "step 6, loss: 25.4433536529541, sharpe ratio: 0.0158060337942143\n",
      "step 7, loss: 25.426921844482422, sharpe ratio: 0.011254613020487005\n",
      "step 8, loss: 25.39236068725586, sharpe ratio: -0.04731188331903314\n",
      "step 9, loss: 25.454936981201172, sharpe ratio: 0.12328751422185592\n",
      "step 10, loss: 25.367897033691406, sharpe ratio: -0.04564459009357893\n",
      "step 11, loss: 25.383546829223633, sharpe ratio: 0.0198068582800585\n",
      "step 12, loss: 25.374385833740234, sharpe ratio: 0.0120594447411819\n",
      "validation sharpe ratio: 0.060830425101331506\n",
      "epoch 84: loss: 25.43478775024414, learning rate: 9.999998292187229e-05\n",
      "epoch 85...\n",
      "step 0, loss: 25.335460662841797, sharpe ratio: -0.0718513914377284\n",
      "step 1, loss: 25.336505889892578, sharpe ratio: -0.033759411184470935\n",
      "step 2, loss: 25.345149993896484, sharpe ratio: 0.010298594636808997\n",
      "step 3, loss: 25.34730339050293, sharpe ratio: 0.016929063399387233\n",
      "step 4, loss: 25.339921951293945, sharpe ratio: 0.028330008443443876\n",
      "step 5, loss: 25.298198699951172, sharpe ratio: -0.04884993835081514\n",
      "step 6, loss: 25.301265716552734, sharpe ratio: 0.022744616638894887\n",
      "step 7, loss: 25.26272964477539, sharpe ratio: -0.06407946997816545\n",
      "step 8, loss: 25.290063858032227, sharpe ratio: 0.038212018288117\n",
      "step 9, loss: 25.300912857055664, sharpe ratio: 0.07139556314248094\n",
      "step 10, loss: 25.270130157470703, sharpe ratio: 0.019450058381338814\n",
      "step 11, loss: 25.249534606933594, sharpe ratio: -0.009018732340734747\n",
      "step 12, loss: 25.281238555908203, sharpe ratio: 0.08278148811492965\n",
      "validation sharpe ratio: 0.05529652804803683\n",
      "epoch 85: loss: 25.30449676513672, learning rate: 9.999998292187229e-05\n",
      "epoch 86...\n",
      "step 0, loss: 25.257339477539062, sharpe ratio: 0.041221283079599456\n",
      "step 1, loss: 25.223182678222656, sharpe ratio: 0.010079306844645051\n",
      "step 2, loss: 25.228187561035156, sharpe ratio: 0.04226536203869104\n",
      "step 3, loss: 25.176006317138672, sharpe ratio: -0.042401620905259986\n",
      "step 4, loss: 25.176301956176758, sharpe ratio: -0.027156428994956044\n",
      "step 5, loss: 25.168399810791016, sharpe ratio: -0.019529622816501974\n",
      "step 6, loss: 25.235401153564453, sharpe ratio: 0.10328551533915944\n",
      "step 7, loss: 25.165658950805664, sharpe ratio: 0.011684979964162342\n",
      "step 8, loss: 25.15421485900879, sharpe ratio: 0.014816990768410053\n",
      "step 9, loss: 25.125486373901367, sharpe ratio: -0.03486647725220411\n",
      "step 10, loss: 25.105972290039062, sharpe ratio: -0.04648874764868162\n",
      "step 11, loss: 25.128921508789062, sharpe ratio: 0.030650243924210795\n",
      "step 12, loss: 25.110769271850586, sharpe ratio: 0.009505462898248985\n",
      "validation sharpe ratio: 0.060830425101331506\n",
      "epoch 86: loss: 25.173526763916016, learning rate: 9.999998292187229e-05\n",
      "epoch 87...\n",
      "step 0, loss: 25.04718780517578, sharpe ratio: -0.09568653333914645\n",
      "step 1, loss: 25.12738037109375, sharpe ratio: 0.05558679926871957\n",
      "step 2, loss: 25.085338592529297, sharpe ratio: 0.024876042037142193\n",
      "step 3, loss: 25.07590675354004, sharpe ratio: 0.015319393962948543\n",
      "step 4, loss: 25.068927764892578, sharpe ratio: 0.019401412475432867\n",
      "step 5, loss: 25.048904418945312, sharpe ratio: 0.0034167878026902278\n",
      "step 6, loss: 25.063919067382812, sharpe ratio: 0.039965278424698825\n",
      "step 7, loss: 25.061172485351562, sharpe ratio: 0.0907561064807714\n",
      "step 8, loss: 25.037683486938477, sharpe ratio: 0.030735882882662054\n",
      "step 9, loss: 24.9757080078125, sharpe ratio: -0.08920034689363185\n",
      "step 10, loss: 24.997373580932617, sharpe ratio: 0.000861055765062137\n",
      "step 11, loss: 25.028907775878906, sharpe ratio: 0.08746291884350071\n",
      "step 12, loss: 24.96211051940918, sharpe ratio: -0.048915986518434124\n",
      "validation sharpe ratio: 0.06172357698396414\n",
      "epoch 87: loss: 25.044654846191406, learning rate: 9.999998292187229e-05\n",
      "epoch 88...\n",
      "step 0, loss: 24.98870277404785, sharpe ratio: 0.0314347749645194\n",
      "step 1, loss: 24.93648338317871, sharpe ratio: -0.044938699923247866\n",
      "step 2, loss: 24.947927474975586, sharpe ratio: 0.025204493616665512\n",
      "step 3, loss: 24.94746971130371, sharpe ratio: 0.009953992307050213\n",
      "step 4, loss: 24.94232177734375, sharpe ratio: 0.023840936428148253\n",
      "step 5, loss: 24.929445266723633, sharpe ratio: 0.013011739305214626\n",
      "step 6, loss: 24.900089263916016, sharpe ratio: -0.016363570685639414\n",
      "step 7, loss: 24.898813247680664, sharpe ratio: 0.0033493014947031663\n",
      "step 8, loss: 24.91806983947754, sharpe ratio: 0.049746978130799355\n",
      "step 9, loss: 24.862869262695312, sharpe ratio: -0.012146402099600186\n",
      "step 10, loss: 24.880552291870117, sharpe ratio: 0.01471377721809031\n",
      "step 11, loss: 24.858654022216797, sharpe ratio: -0.003087434554123791\n",
      "step 12, loss: 24.80451202392578, sharpe ratio: -0.07488591411339547\n",
      "validation sharpe ratio: 0.06756464256052191\n",
      "epoch 88: loss: 24.90891456604004, learning rate: 9.999998292187229e-05\n",
      "epoch 89...\n",
      "step 0, loss: 24.805198669433594, sharpe ratio: -0.07420907985522143\n",
      "step 1, loss: 24.811901092529297, sharpe ratio: -0.03399505538479029\n",
      "step 2, loss: 24.817628860473633, sharpe ratio: -0.008083955611541084\n",
      "step 3, loss: 24.812875747680664, sharpe ratio: 0.016357023242728516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4, loss: 24.804763793945312, sharpe ratio: 0.0023349188689984687\n",
      "step 5, loss: 24.813339233398438, sharpe ratio: 0.056013640054745475\n",
      "step 6, loss: 24.751394271850586, sharpe ratio: -0.04457745767560539\n",
      "step 7, loss: 24.748802185058594, sharpe ratio: -0.028794286067555925\n",
      "step 8, loss: 24.729827880859375, sharpe ratio: -0.026450546708027656\n",
      "step 9, loss: 24.73385238647461, sharpe ratio: -0.03813749142822384\n",
      "step 10, loss: 24.71689796447754, sharpe ratio: -0.03723051124056666\n",
      "step 11, loss: 24.71986198425293, sharpe ratio: -0.019719210547659682\n",
      "step 12, loss: 24.701961517333984, sharpe ratio: -0.036682834298397866\n",
      "validation sharpe ratio: 0.06756464256052191\n",
      "epoch 89: loss: 24.76679229736328, learning rate: 9.999998292187229e-05\n",
      "epoch 90...\n",
      "step 0, loss: 24.71565818786621, sharpe ratio: -0.005188970203047559\n",
      "step 1, loss: 24.694622039794922, sharpe ratio: -0.013861935165155596\n",
      "step 2, loss: 24.67151641845703, sharpe ratio: -0.04472202872816361\n",
      "step 3, loss: 24.668161392211914, sharpe ratio: -0.03437651267748756\n",
      "step 4, loss: 24.686635971069336, sharpe ratio: 0.03468661061430478\n",
      "step 5, loss: 24.66972541809082, sharpe ratio: 0.014245281729701749\n",
      "step 6, loss: 24.655065536499023, sharpe ratio: 0.022510158115686133\n",
      "step 7, loss: 24.66230583190918, sharpe ratio: 0.05845973712707157\n",
      "step 8, loss: 24.62491226196289, sharpe ratio: -0.01331304412546168\n",
      "step 9, loss: 24.599424362182617, sharpe ratio: -0.04349013522540271\n",
      "step 10, loss: 24.5959415435791, sharpe ratio: -0.02720795685648227\n",
      "step 11, loss: 24.61646270751953, sharpe ratio: 0.03704006463824677\n",
      "step 12, loss: 24.61075782775879, sharpe ratio: 0.031231759415346352\n",
      "validation sharpe ratio: 0.06073724402161204\n",
      "epoch 90: loss: 24.651628494262695, learning rate: 9.999998292187229e-05\n",
      "epoch 91...\n",
      "step 0, loss: 24.58721923828125, sharpe ratio: 0.02945600977099628\n",
      "step 1, loss: 24.539813995361328, sharpe ratio: -0.07117163309693873\n",
      "step 2, loss: 24.573623657226562, sharpe ratio: 0.017251099156365864\n",
      "step 3, loss: 24.53824234008789, sharpe ratio: -0.020512006228777725\n",
      "step 4, loss: 24.544649124145508, sharpe ratio: 0.01404013761036093\n",
      "step 5, loss: 24.55434226989746, sharpe ratio: 0.05651856368416167\n",
      "step 6, loss: 24.535905838012695, sharpe ratio: 0.03397978349875517\n",
      "step 7, loss: 24.541839599609375, sharpe ratio: 0.06014552979721982\n",
      "step 8, loss: 24.52275276184082, sharpe ratio: 0.03689459903713896\n",
      "step 9, loss: 24.488229751586914, sharpe ratio: -0.01307479041906955\n",
      "step 10, loss: 24.4703311920166, sharpe ratio: -0.025885901483348794\n",
      "step 11, loss: 24.468198776245117, sharpe ratio: 0.001105392611510811\n",
      "step 12, loss: 24.467370986938477, sharpe ratio: 0.01749598205974866\n",
      "validation sharpe ratio: 0.059709796586728904\n",
      "epoch 91: loss: 24.525577545166016, learning rate: 9.999998292187229e-05\n",
      "epoch 92...\n",
      "step 0, loss: 24.473875045776367, sharpe ratio: 0.0361688732315866\n",
      "step 1, loss: 24.41087532043457, sharpe ratio: -0.07075435236560143\n",
      "step 2, loss: 24.437089920043945, sharpe ratio: 0.007083041572793585\n",
      "step 3, loss: 24.424968719482422, sharpe ratio: -0.003415201868404781\n",
      "step 4, loss: 24.432024002075195, sharpe ratio: 0.03830943294436287\n",
      "step 5, loss: 24.42896842956543, sharpe ratio: 0.05121956437913307\n",
      "step 6, loss: 24.398357391357422, sharpe ratio: 0.02389147117186823\n",
      "step 7, loss: 24.374467849731445, sharpe ratio: -0.027631675964951755\n",
      "step 8, loss: 24.352893829345703, sharpe ratio: -0.04252830011278846\n",
      "step 9, loss: 24.407453536987305, sharpe ratio: 0.08264309689023223\n",
      "step 10, loss: 24.341590881347656, sharpe ratio: -0.022365025790945944\n",
      "step 11, loss: 24.32892417907715, sharpe ratio: -0.03572257318635\n",
      "step 12, loss: 24.329505920410156, sharpe ratio: -0.005867000726265623\n",
      "validation sharpe ratio: 0.06372228650483343\n",
      "epoch 92: loss: 24.39546012878418, learning rate: 9.999998292187229e-05\n",
      "epoch 93...\n",
      "step 0, loss: 24.357255935668945, sharpe ratio: 0.054060554007347494\n",
      "step 1, loss: 24.337352752685547, sharpe ratio: 0.027849528627810153\n",
      "step 2, loss: 24.32901954650879, sharpe ratio: 0.04245896696226993\n",
      "step 3, loss: 24.30977439880371, sharpe ratio: 0.025491750077551802\n",
      "step 4, loss: 24.314268112182617, sharpe ratio: 0.061482874468333164\n",
      "step 5, loss: 24.256614685058594, sharpe ratio: -0.043444888044759884\n",
      "step 6, loss: 24.255027770996094, sharpe ratio: -0.02578738359155243\n",
      "step 7, loss: 24.2801513671875, sharpe ratio: 0.058038981571165174\n",
      "step 8, loss: 24.22220230102539, sharpe ratio: -0.040921464486713785\n",
      "step 9, loss: 24.213621139526367, sharpe ratio: -0.046356047891602414\n",
      "step 10, loss: 24.201562881469727, sharpe ratio: -0.04845852879160705\n",
      "step 11, loss: 24.270713806152344, sharpe ratio: 0.1152869640043343\n",
      "step 12, loss: 24.225156784057617, sharpe ratio: 0.04283144506792678\n",
      "validation sharpe ratio: 0.062170553676469266\n",
      "epoch 93: loss: 24.27482795715332, learning rate: 9.999998292187229e-05\n",
      "epoch 94...\n",
      "step 0, loss: 24.255380630493164, sharpe ratio: 0.10002017103954829\n",
      "step 1, loss: 24.19169807434082, sharpe ratio: 0.04312573186794285\n",
      "step 2, loss: 24.16313362121582, sharpe ratio: -0.02028800043681634\n",
      "step 3, loss: 24.14411735534668, sharpe ratio: -0.039941488945379106\n",
      "step 4, loss: 24.164817810058594, sharpe ratio: 0.016847942800012845\n",
      "step 5, loss: 24.1578426361084, sharpe ratio: -0.011491937805346075\n",
      "step 6, loss: 24.139604568481445, sharpe ratio: 0.04554436991562849\n",
      "step 7, loss: 24.136716842651367, sharpe ratio: 0.03145319251391354\n",
      "step 8, loss: 24.116167068481445, sharpe ratio: 0.0007353576367420192\n",
      "step 9, loss: 24.087087631225586, sharpe ratio: -0.03432023049356636\n",
      "step 10, loss: 24.10555648803711, sharpe ratio: 0.01897533972414256\n",
      "step 11, loss: 24.10905647277832, sharpe ratio: 0.054036793413531126\n",
      "step 12, loss: 24.04692840576172, sharpe ratio: -0.05573618586302102\n",
      "validation sharpe ratio: 0.06594864114364682\n",
      "epoch 94: loss: 24.139854431152344, learning rate: 9.999998292187229e-05\n",
      "epoch 95...\n",
      "step 0, loss: 24.046958923339844, sharpe ratio: -0.030716986734664328\n",
      "step 1, loss: 24.03964614868164, sharpe ratio: -0.013159532036055153\n",
      "step 2, loss: 24.06266212463379, sharpe ratio: 0.027670028024827667\n",
      "step 3, loss: 24.07578468322754, sharpe ratio: 0.07145745497977429\n",
      "step 4, loss: 24.00545883178711, sharpe ratio: -0.05946623835227418\n",
      "step 5, loss: 24.0139217376709, sharpe ratio: -0.001585908286609098\n",
      "step 6, loss: 24.041048049926758, sharpe ratio: 0.07006836711984266\n",
      "step 7, loss: 23.978017807006836, sharpe ratio: -0.03494377249183796\n",
      "step 8, loss: 24.027210235595703, sharpe ratio: 0.06775892413643674\n",
      "step 9, loss: 23.96900749206543, sharpe ratio: -0.02009036749601186\n",
      "step 10, loss: 23.981781005859375, sharpe ratio: 0.03142477827470118\n",
      "step 11, loss: 23.94388771057129, sharpe ratio: -0.03323164013873173\n",
      "step 12, loss: 23.94870376586914, sharpe ratio: 0.02096581636724883\n",
      "validation sharpe ratio: 0.0634548058058184\n",
      "epoch 95: loss: 24.010313034057617, learning rate: 9.999998292187229e-05\n",
      "epoch 96...\n",
      "step 0, loss: 23.908668518066406, sharpe ratio: -0.07960912458243492\n",
      "step 1, loss: 23.95462989807129, sharpe ratio: 0.05509552073466955\n",
      "step 2, loss: 23.924503326416016, sharpe ratio: 0.016253082407494145\n",
      "step 3, loss: 23.92129898071289, sharpe ratio: 0.013883410634346241\n",
      "step 4, loss: 23.909011840820312, sharpe ratio: 0.00982164268723645\n",
      "step 5, loss: 23.884567260742188, sharpe ratio: -0.022338986086601828\n",
      "step 6, loss: 23.877559661865234, sharpe ratio: -0.007576786088821466\n",
      "step 7, loss: 23.866628646850586, sharpe ratio: -0.010773972354905655\n",
      "step 8, loss: 23.845550537109375, sharpe ratio: -0.039753367543036044\n",
      "step 9, loss: 23.846567153930664, sharpe ratio: 0.008457356380814324\n",
      "step 10, loss: 23.85574722290039, sharpe ratio: 0.03266563916461579\n",
      "step 11, loss: 23.768369674682617, sharpe ratio: -0.13252919646001063\n",
      "step 12, loss: 23.85508918762207, sharpe ratio: 0.07392482547211432\n",
      "validation sharpe ratio: 0.06071461482567184\n",
      "epoch 96: loss: 23.878324508666992, learning rate: 9.999998292187229e-05\n",
      "epoch 97...\n",
      "step 0, loss: 23.839136123657227, sharpe ratio: 0.06213050253217134\n",
      "step 1, loss: 23.80341911315918, sharpe ratio: -0.0019122136191965424\n",
      "step 2, loss: 23.775772094726562, sharpe ratio: -0.02918968529419347\n",
      "step 3, loss: 23.811603546142578, sharpe ratio: 0.052766749291899284\n",
      "step 4, loss: 23.754289627075195, sharpe ratio: -0.06145956418918337\n",
      "step 5, loss: 23.76508140563965, sharpe ratio: -0.014868555274857975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6, loss: 23.744699478149414, sharpe ratio: -0.013496929096475735\n",
      "step 7, loss: 23.762676239013672, sharpe ratio: 0.05226058889951503\n",
      "step 8, loss: 23.739683151245117, sharpe ratio: -0.024688484760127596\n",
      "step 9, loss: 23.729305267333984, sharpe ratio: 0.009229917318539048\n",
      "step 10, loss: 23.724077224731445, sharpe ratio: 0.011248822535170065\n",
      "step 11, loss: 23.676040649414062, sharpe ratio: -0.04608373784311101\n",
      "step 12, loss: 23.72764778137207, sharpe ratio: 0.05203527191999799\n",
      "validation sharpe ratio: 0.06226592758061642\n",
      "epoch 97: loss: 23.757957458496094, learning rate: 9.999998292187229e-05\n",
      "epoch 98...\n",
      "step 0, loss: 23.69357681274414, sharpe ratio: 0.0025764858228620735\n",
      "step 1, loss: 23.724084854125977, sharpe ratio: 0.07009292752030896\n",
      "step 2, loss: 23.657455444335938, sharpe ratio: -0.031711557336503396\n",
      "step 3, loss: 23.652830123901367, sharpe ratio: -0.019653601587822693\n",
      "step 4, loss: 23.645898818969727, sharpe ratio: -0.010840885327512557\n",
      "step 5, loss: 23.668365478515625, sharpe ratio: 0.043760751859120456\n",
      "step 6, loss: 23.612077713012695, sharpe ratio: -0.05077702812988986\n",
      "step 7, loss: 23.580896377563477, sharpe ratio: -0.07543522835799477\n",
      "step 8, loss: 23.61564064025879, sharpe ratio: 0.020181070949814583\n",
      "step 9, loss: 23.56485366821289, sharpe ratio: -0.08235163275651368\n",
      "step 10, loss: 23.559574127197266, sharpe ratio: -0.063293738777947\n",
      "step 11, loss: 23.553573608398438, sharpe ratio: -0.053867285829396754\n",
      "step 12, loss: 23.565380096435547, sharpe ratio: -0.018373808421944325\n",
      "validation sharpe ratio: 0.06045120320618695\n",
      "epoch 98: loss: 23.622629165649414, learning rate: 9.999998292187229e-05\n",
      "epoch 99...\n",
      "step 0, loss: 23.534055709838867, sharpe ratio: -0.047163992778665675\n",
      "step 1, loss: 23.553831100463867, sharpe ratio: 0.013862263852476055\n",
      "step 2, loss: 23.538217544555664, sharpe ratio: -0.006129457348260498\n",
      "step 3, loss: 23.577190399169922, sharpe ratio: 0.05690942456093798\n",
      "step 4, loss: 23.503318786621094, sharpe ratio: -0.02324610377597506\n",
      "step 5, loss: 23.52412223815918, sharpe ratio: 0.01710506989815904\n",
      "step 6, loss: 23.51106071472168, sharpe ratio: 0.01477772971807803\n",
      "step 7, loss: 23.509292602539062, sharpe ratio: 0.03572395151670523\n",
      "step 8, loss: 23.502002716064453, sharpe ratio: 0.028149868336772053\n",
      "step 9, loss: 23.472951889038086, sharpe ratio: -0.013988926145419436\n",
      "step 10, loss: 23.450668334960938, sharpe ratio: -0.04799389908393109\n",
      "step 11, loss: 23.477773666381836, sharpe ratio: 0.057349941856098346\n",
      "step 12, loss: 23.45940399169922, sharpe ratio: 0.028077124055453057\n",
      "validation sharpe ratio: 0.06045120320618695\n",
      "epoch 99: loss: 23.508760452270508, learning rate: 9.999998292187229e-05\n"
     ]
    }
   ],
   "source": [
    "agent.train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hist = pd.read_csv('PG/logs/history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>loss</th>\n",
       "      <th>lr</th>\n",
       "      <th>val_sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58.017525</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.023404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>57.798794</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.025050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>57.558113</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.018719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>57.342892</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.018285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>57.100552</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.029272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>56.895096</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.041137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>56.656227</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.024203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>56.425014</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.027736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>56.220291</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.029758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>55.982365</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.015775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>55.767151</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.029846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>55.550289</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.026090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>55.334023</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.023029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>55.093403</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>54.894669</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.025981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>54.670494</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.031714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>54.453205</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>54.238243</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.032544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>54.026260</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.030990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>53.809326</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.030681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>53.596470</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.032372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>53.364254</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.046621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>53.164211</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.049760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>52.935955</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.034340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>52.733311</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.034702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>52.494682</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.055491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>52.298992</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.055521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>52.103367</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.041088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>51.894150</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.034651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>51.665867</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>-0.037763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>43.650204</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.018059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>43.455235</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.011159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>43.265038</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.019473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>43.092754</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.009479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>42.912907</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.011339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>42.725548</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.008970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>42.558327</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.017911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>42.355263</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.018949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>42.183208</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.046218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>42.008816</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.038143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>41.824406</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.045619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>41.637138</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.044589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>41.477268</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.047202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>41.304062</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.034665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>41.116447</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.034665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>40.954002</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.048597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>40.771408</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.049344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>40.606083</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.060363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>40.439228</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>40.240845</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.062915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>40.077744</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.060135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>39.905312</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.043262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>39.725655</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.048989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>39.563950</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.043384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>39.388229</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.048861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>39.206596</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.053460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>39.043938</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.044352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>38.876072</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.050999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>38.703182</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.047540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>38.542423</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.065708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0       loss      lr  val_sharpe\n",
       "0            0  58.017525  0.0001   -0.023404\n",
       "1            1  57.798794  0.0001   -0.025050\n",
       "2            2  57.558113  0.0001   -0.018719\n",
       "3            3  57.342892  0.0001   -0.018285\n",
       "4            4  57.100552  0.0001   -0.029272\n",
       "5            5  56.895096  0.0001   -0.041137\n",
       "6            6  56.656227  0.0001   -0.024203\n",
       "7            7  56.425014  0.0001   -0.027736\n",
       "8            8  56.220291  0.0001   -0.029758\n",
       "9            9  55.982365  0.0001   -0.015775\n",
       "10          10  55.767151  0.0001   -0.029846\n",
       "11          11  55.550289  0.0001   -0.026090\n",
       "12          12  55.334023  0.0001   -0.023029\n",
       "13          13  55.093403  0.0001   -0.032983\n",
       "14          14  54.894669  0.0001   -0.025981\n",
       "15          15  54.670494  0.0001   -0.031714\n",
       "16          16  54.453205  0.0001   -0.028400\n",
       "17          17  54.238243  0.0001   -0.032544\n",
       "18          18  54.026260  0.0001   -0.030990\n",
       "19          19  53.809326  0.0001   -0.030681\n",
       "20          20  53.596470  0.0001   -0.032372\n",
       "21          21  53.364254  0.0001   -0.046621\n",
       "22          22  53.164211  0.0001   -0.049760\n",
       "23          23  52.935955  0.0001   -0.034340\n",
       "24          24  52.733311  0.0001   -0.034702\n",
       "25          25  52.494682  0.0001   -0.055491\n",
       "26          26  52.298992  0.0001   -0.055521\n",
       "27          27  52.103367  0.0001   -0.041088\n",
       "28          28  51.894150  0.0001   -0.034651\n",
       "29          29  51.665867  0.0001   -0.037763\n",
       "..         ...        ...     ...         ...\n",
       "70          70  43.650204  0.0001    0.018059\n",
       "71          71  43.455235  0.0001    0.011159\n",
       "72          72  43.265038  0.0001    0.019473\n",
       "73          73  43.092754  0.0001    0.009479\n",
       "74          74  42.912907  0.0001    0.011339\n",
       "75          75  42.725548  0.0001    0.008970\n",
       "76          76  42.558327  0.0001    0.017911\n",
       "77          77  42.355263  0.0001    0.018949\n",
       "78          78  42.183208  0.0001    0.046218\n",
       "79          79  42.008816  0.0001    0.038143\n",
       "80          80  41.824406  0.0001    0.045619\n",
       "81          81  41.637138  0.0001    0.044589\n",
       "82          82  41.477268  0.0001    0.047202\n",
       "83          83  41.304062  0.0001    0.034665\n",
       "84          84  41.116447  0.0001    0.034665\n",
       "85          85  40.954002  0.0001    0.048597\n",
       "86          86  40.771408  0.0001    0.049344\n",
       "87          87  40.606083  0.0001    0.060363\n",
       "88          88  40.439228  0.0001    0.061224\n",
       "89          89  40.240845  0.0001    0.062915\n",
       "90          90  40.077744  0.0001    0.060135\n",
       "91          91  39.905312  0.0001    0.043262\n",
       "92          92  39.725655  0.0001    0.048989\n",
       "93          93  39.563950  0.0001    0.043384\n",
       "94          94  39.388229  0.0001    0.048861\n",
       "95          95  39.206596  0.0001    0.053460\n",
       "96          96  39.043938  0.0001    0.044352\n",
       "97          97  38.876072  0.0001    0.050999\n",
       "98          98  38.703182  0.0001    0.047540\n",
       "99          99  38.542423  0.0001    0.065708\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 978,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb3b8e3e48>]"
      ]
     },
     "execution_count": 1025,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_hist['loss'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb40130c50>]"
      ]
     },
     "execution_count": 979,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_hist['val_sharpe'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.load_weights('PG/logs/model_epoch99.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.num_val = 600\n",
    "agent.num_test = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.arange(agent.num_train + agent.num_val, agent.num_train + agent.num_val + agent.num_test).reshape((-1, agent.traj_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb3b9fc4e0>]"
      ]
     },
     "execution_count": 1017,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data['Close'].values[np.arange(agent.num_train + agent.num_val, agent.num_train + agent.num_val + agent.num_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharpe ratio if all the actions are long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009819118260673568"
      ]
     },
     "execution_count": 1018,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.sharpe_ratio(np.arange(agent.num_train + agent.num_val, agent.num_train + agent.num_val + agent.num_test), np.ones(agent.num_test, dtype = int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharpe ratio using the strategy predicted by the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation sharpe ratio: 0.11079143590554417\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.11079143590554417"
      ]
     },
     "execution_count": 1019,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.eva(test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = agent.predict(agent.data_scaled[test_indices]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [agent.action_space[i] for i in actions]\n",
    "a = np.array(a[:-1])\n",
    "ret = np.diff(np.log(agent.data_raw[test_indices.flatten()][:,3])) * a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcb3b90b3c8>]"
      ]
     },
     "execution_count": 1024,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(np.array(ret)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
